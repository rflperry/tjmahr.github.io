---
title: Recent slides on Bayesian regression
excerpt: This time in LaTeX
tags:
  - bayesian

---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Back in April, I gave a guest lecture on Bayesian regression for the psychology 
department's graduate regression class. This is the same course where I first
learned regression---and where I first started using R for statistics, instead
of data cleaning---so it was a fun experience. I also gave the lecture to my lab
here, but it was more a guided discussion through the slides. It was just a
handful of us, and we could really talk through some the big ideas and
intuitions.

Here are the materials:

* [PDF version of slides](https://cdn.rawgit.com/tjmahr/Psych710_BayesLecture/55f446a0/bayes_slides_out.pdf)
* [Same content but as a (non-presentation) web page](https://cdn.rawgit.com/tjmahr/Psych710_BayesLecture/55f446a0/bayes_slides.html)
* [Github repository](https://github.com/tjmahr/Psych710_BayesLecture)

## Observations (training data)

As I did with [my last Bayes talk](/rstanarm-tutorial-slides/), I'm going to note 
some questions from the audience, so I don't forget what kinds of questions
people have when they are introduced to Bayesian statistics.

One theme was frequentist baggage :handbag:. One person asked about 
[Type I and Type II error](https://twitter.com/chrisalbon/status/874683365865029632) 
rates. I did not have a satisfactory (that is, _rehearsed_) answer ready for
this question. I think I said something about how those terms are based on a
frequentist, repeated-sampling paradigm, whereas a Bayesian approach 
[worries about different sorts of errors](http://andrewgelman.com/2004/12/29/type_1_type_2_t/). 
(Statistical [power is still important](https://alexanderetz.com/2015/05/21/type-s-and-type-m-errors/), 
of course, for both approaches.) Next time, I should study up on the
frequentist properties of Bayesian models, so I can field these questions
better. 

Other questions:

* Another bit of frequentist baggage :handbag:. I mentioned that with a
posterior predictive distribution, we can put an uncertainty interval on any
statistic we can calculate, and this point brought up the question of _multiple
comparisons_. These are a bad thing in classical statistics. But for Bayes, there
is only one model, and the multiple comparisons are really only the implications 
of one model.
* Someone else said that they had heard that Bayesian models can provide evidence
for a null effect---*how does that work?* I briefly described the 
[ROPE approach](https://doingbayesiandataanalysis.blogspot.nl/2016/12/bayesian-assessment-of-null-values.html),
ignoring the existence of Bayes factors entirely.

For future iterations of this tutorial, I should have a worked example, maybe a
blog post, on each of these issues. 

A big part of my enthusiasm for Bayesian statistics is that I find it much more intuitive than frequentist statistics. _Yes!_ I thought to myself. _I never have to worry about whether I am correctly remembering the technically correct definition of _p*-value_ ever again!* Well actually no. I need to know this stuff even better than ever if I am going to teach on it. 
