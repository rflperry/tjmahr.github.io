---
title: Plotting partial pooling in mixed effect models
excerpt: Wow, there are a lot of bilabial sounds in that title
tags:
  - rstanarm
  - bayesian
  - r
  - mixed effects
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
asp_facet <- 1
```

In this post, I demonstrate a few techniques for plotting information 
from a relatively simple mixed-effects model. These plots can help us develop
intuitions about what these models are doing and what "partial pooling" means.

## The `sleepstudy` dataset

For these examples, I'm going to use the `sleepstudy` dataset from the lme4
package. The outcome measure is reaction time, the predictor measure is days of
sleep deprivation, and these measurements are nested within participants---we
have 10 observations per participant. I am also going to add two fake
participants with incomplete data to illustrate partial pooling.

```{r}
library(lme4)
library(dplyr)

# Convert to tibble for better printing. Convert factors to strings
sleepstudy <- sleepstudy %>% 
  as_tibble() %>% 
  mutate(Subject = as.character(Subject))

# Add two fake participants
df_sleep <- bind_rows(
  sleepstudy,
  data_frame(Reaction = c(286, 288), Days = 0:1, Subject = "374"),
  data_frame(Reaction = 245, Days = 0, Subject = "373"))

df_sleep
```

We can visualize all the data in ggplot2 by using `facet_wrap()` to create
sub-plots for each participant and `stat_smooth()` to create a regression line
in plotting panel.

```{r facet-plot, fig.asp = .9}
library(ggplot2)

xlab <- "Days of sleep deprivation"
ylab <- "Average reaction time (ms)"

ggplot(df_sleep) + 
  aes(x = Days, y = Reaction) + 
  stat_smooth(method = "lm", se = FALSE, ) +
  # points on top of lines
  geom_point() +
  facet_wrap("Subject") +
  labs(x = xlab, y = ylab) + 
  # We also need to help the x-axis, so it doesn't 
  # create gridlines/ticks on 2.5 days
  scale_x_continuous(breaks = 0:4 * 2)
```

ggplot2 doesn't draw the regressions lines outside of the range of the data (as
in the data for 374) unless we set `fullrange = TRUE`. That's a helpful feature 
for 374.


## Complete pooling and no pooling models

Each one of these panels plotted above shows an independently estimated
regression line. This approach to fitting a separate line for each participant
is sometimes called the **no pooling** model because none of the information
from different participants is combined or _pooled_ together.

We fit a separate line for each cluster of data, unaware
that any of the other participants exist. The `lmList()` function in `lme4`
automates this process.

```{r, warning = FALSE}
df_no_pooling <- lmList(Reaction ~ Days | Subject, df_sleep) %>% 
  coef() %>% 
  # Subject IDs are stored as row-names. Make them an explicit column
  tibble::rownames_to_column("Subject") %>% 
  rename(Intercept = `(Intercept)`, Slope_Days = Days) %>% 
  tibble::add_column(Model = "No pooling") %>% 
  # Remove the participant who only had one data-point
  filter(Subject != "373")

head(df_no_pooling)
```

In contrast, we might consider a **complete pooling** model where all the
information from the participants is combined together. We fit a single line for
the combined data set, unaware that the data came from different participants.

```{r}
# Fit a model on all the data pooled together
m_pooled <- lm(Reaction ~ Days, df_sleep) 

# Repeat the intercept and slope terms for each participant
df_pooled <- data_frame(
  Model = "Complete pooling",
  Subject = unique(df_sleep$Subject),
  Intercept = coef(m_pooled)[1], 
  Slope_Days = coef(m_pooled)[2])

head(df_pooled)
```

We can compare these two approaches. Instead of calculating the regression lines
with `stat_smooth()`, we can use  `geom_abline()` to draw the lines from our 
dataframe with intercept and slope parameters.

```{r pooling-vs-no-pooling, fig.asp = asp_facet}
# Join the raw data so we can use plot the points and the lines.
df_models <- bind_rows(df_pooled, df_no_pooling) %>% 
  left_join(df_sleep, by = "Subject")

p_model_comparison <- ggplot(df_models) + 
  aes(x = Days, y = Reaction) + 
  # Set the color mapping in this layer so the points don't get a color
  geom_abline(aes(intercept = Intercept, slope = Slope_Days, color = Model),
              size = .75) + 
  geom_point() +
  facet_wrap("Subject") +
  labs(x = xlab, y = ylab) + 
  scale_x_continuous(breaks = 0:4 * 2) + 
  # Fix the color palette 
  scale_color_brewer(palette = "Dark2") + 
  theme(legend.position = "top")

p_model_comparison
```

The no pooling model estimates a single line, and we see that same line drawn on
every facet One advantage is that the model can make a guess about the line for 
373 who only one observation. That model looks pretty terrible else, because
nobody is perfectly average. In contrast, the complete pooling model can follow
the data, fitting the sharp trend upwards in 308 and even capture the negative
slope in 335. 

The model cannot make a guess about 373. In [_Statistical
Rethinking_](http://xcelab.net/rm/statistical-rethinking/), McElreath says these
no pooling models have amnesia :hushed::

> Many statistical models also have anterograde amnesia. As the models move from
one cluster—individual, group, location—in the data to another, estimating
parameters for each cluster, they forget everything about the previous clusters.
They behave this way, because the assumptions force them to. Any of the models
from previous chapters that used dummy variables (page 152) to handle categories
are programmed for amnesia. These models implicitly assume that nothing learned
about any one category informs estimates for the other categories—the parameters
are independent of one another and learn from completely separate portions of
the data. This would be like forgetting you had ever been in a café, each time 
you go to a new café. Cafés do differ, but they are also alike.

Once the no pooling model draws the line for 372, and it completely forget
everything it has seen and moves on to 373. It has to skip 373 because it cannot
estimate a line from a single point, and it moves on.

Here's a fun question: Which approach has the better guess for 374's line?


## Incorporating estimates from a mixed-effects model

We can do better with mixed-effects models. In these models, we pool information
from all the lines together to improve our estimates of each individual line. In
particular, after seeing the 18 trend lines for the participants with complete
data, we can make an informed guess about the trend lines for the two
participants with incomplete data. This approach is sometimes called **partial
pooling**. 

We can fit a classical mixed-effects model with the lme4 package:

```{r}
m <- lmer(Reaction ~ 1 + Days + (1 + Days | Subject), df_sleep)
arm::display(m)
```

The first two `coef.est` are the "fixed effects" estimates; they reflect the 
average intercept and slope parameters. For this example, the values are 
practically the same as the complete-pooling estimates. This model assumes that 
each participant's individuals intercept and slope parameters are deviations 
from this average, and these random deviations drawn from a distribution of 
possible intercept and slope parameters. These are "randomly varying" or "random
effects". The information in the `Error terms` area describes information about
the distribution of the effects. Because we have both fixed and random effects, 
we have a "mixed-effects" model. Hence the name.

To visualize these estimates, we extract each participant's intercept and slope
using `coef()`.

```{r}
# Make a dataframe with the fitted effects
df_partial_pooling <- coef(m)[["Subject"]] %>% 
  as_tibble() %>% 
  tibble::rownames_to_column("Subject") %>% 
  rename(Intercept = `(Intercept)`, Slope_Days = Days) %>% 
  mutate(Model = "Partial pooling")
head(df_partial_pooling)
```

```{r partial-pooling-vs-others, fig.asp = asp_facet}
df_models <- bind_rows(df_pooled, df_no_pooling, df_partial_pooling) %>% 
  left_join(df_sleep, by = "Subject")

# Replace the data-set of the last plot
p_model_comparison %+% df_models
```

Most of the time, the no pooling and partial pooling lines are on top of each
other. But when the two differ, it's because the partial pooled model's line is
pulled slightly towards the no-pooling line.

We can appreciate the differences by zooming on some participants.

```{r zoomed-in-partial-pooling}
df_zoom <- df_models %>% 
  filter(Subject %in% c("335", "350", "373", "374"))

p_model_comparison %+% df_zoom
```

The negative line for 335 from the no pooling model gets a flatter slope in the
partial pooling model. The model knows that negative trends are rather unlikely,
so the it hedges its bets and pulled that line towards the group average.
Something similar happens with 350 where a sharp slope is slightly attenuated.
For the participants with incomplete data, the model hedges its bets. The
complete pooling and the partial pooling lines are basically parallel---i.e,
they have the same slope. That's a reasonable guess given so little information.


## Shrinkage

The partial pooling model pulls more extreme estimates towards an overall 
average. We can visualize this effect by plotting a scatterplot of intercept and
slope parameters from each model, and connecting estimates for the same
participant.

```{r shrinkage-plot}
# Also visualized the point for the fixed effects
df_fixef <- data_frame(
  Model = "Partial pooling (average)",
  Intercept = fixef(m)[1],
  Slope_Days = fixef(m)[2])

# Complete pooling / fixed effects are center of gravity in the plot
df_gravity <- df_pooled %>% 
  distinct(Model, Intercept, Slope_Days) %>% 
  bind_rows(df_fixef)
df_gravity

df_pulled <- bind_rows(df_no_pooling, df_partial_pooling)

ggplot(df_pulled) + 
  aes(x = Intercept, y = Slope_Days, color = Model) + 
  # Draw an arrow connecting the observations between models
  geom_path(aes(group = Subject, color = NULL), 
            arrow = arrow(length = unit(.02, "npc"))) + 
  # Layer the points next so they land on the arrowheards
  geom_point(data = df_gravity, size = 5) + 
  geom_point(size = 2) + 
  # Use ggrepel to jitter the labels away from the points
  ggrepel::geom_text_repel(
    aes(label = Subject, color = NULL), 
    data = df_no_pooling) + 
  # Don't forget 373
  ggrepel::geom_text_repel(
    aes(label = Subject, color = NULL), 
    data = filter(df_partial_pooling, Subject == "373")) + 
  theme(legend.position = "bottom") + 
  ggtitle("Pooling of regression parameters") + 
  xlab("Intercept estimate") + 
  ylab("Slope estimate") + 
  scale_color_brewer(palette = "Dark2") 
```

The average intercept and slope act like a center of gravity, pulling values
parameter estimates towards it. Hmm, maybe gravity is not quite the right
analogy, because the pull is greater for more extreme values. The lines near
that center point are very short; they get adjusted very little. But more The
lines _in general_ get longer as we move away from the complete pooling
estimate. The fewer the observations in a cluster (here, participants), the more
information is borrowed from other clusters, and the greater the pull towards
the average estimate. Participant 373 had only observation, so their slope
estimate is the average. Likewise, 374 had only two observations, so they get
pulled the farthest and receive a slope estimate near the overall average.

This effect is sometimes called _shrinkage_, because more extreme values
shrinkage are pulled towards a more reasonable, more average value. In the lme4
book, Douglas Bate provides an alternative to _shrinkage_:

> The term "shrinkage" may have negative connotations. John Tukey preferred to
refer to the process as the estimates for individual subjects "borrowing
strength" from each other. This is a fundamental difference in the models
underlying mixed-effects models versus strictly fixed effects models. In a
mixed-effects model we assume that the levels of a grouping factor are a
selection from a population and, as a result, can be expected to share
characteristics to some degree. Consequently, the predictions from a
mixed-effects model are attenuated relative to those from strictly fixed-effects
models.

Another term would be _regularization_ if we think about how the model avoids
overfitting by the taming extreme estimates.





## A topographic map of parameters

For the next visualization, we are going to visualize the distribution of
random-varying effects. (I am partly including it so that I have a working
ggplot2 version of how to make this plot online.)

I already remarked that the model estimates a distribution of intercept and 
slope effects. We know where the center of that distribution is: It's the fixed 
effects estimate, the center of gravity in the last plot. What the model also 
needs to estimate is the variability/spread of values around that center. Also,
intercept and slopes might be correlated: Maybe the effect of an additional day
on reaction time is diminished for participants who are slower to respond in
general. So, the model also estimates the correlation of those effects too.

Imagine that the last plot was landscape, and fixed effects point is the peak of
a hill. What were are going to do is draw a topographic map with contour lines
to show different elevation regions on that hill.

First, we need to create the covariance matrix estimated by the model.

```{r}
# Extract the matrix
cov_mat <- VarCorr(m, )[["Subject"]]

# Strip off some details so that just the useful part is printed
attr(cov_mat, "stddev") <- NULL
attr(cov_mat, "correlation") <- NULL
cov_mat
```

The `ellipse()` function takes a covariance matrix, a center value, and
quantile/confidence level and returns the points from an oval around the center
at the given confidence level. We create five different ellipses for different 
quantile levels.

```{r}
library(ellipse)

# Helper function to make a data-frame of ellipse points that includes the level
# as a column
make_ellipse <- function(cov_mat, center, level) {
  ellipse(cov_mat, centre = center, level = level) %>%
    as.data.frame() %>%
    tibble::add_column(level = level) %>% 
    tibble::as_tibble()
}

center <- fixef(m)
levels <- c(.1, .3, .5, .7, .9)

# Create an ellipse dataframe for each of the levels defined above and 
# combined them all together 
df_ellipse <- levels %>%
  purrr::map_df(~ make_ellipse(cov_mat, center, level = .x)) %>% 
  rename(Intercept = `(Intercept)`, Slope_Days = Days)

df_ellipse
```


To visualize the scale of variability and correlation in the random

To describe a
distribution--think of a bell curve--you need to know the center and the
scale/width of the distribution. The center here are the fixed effects
estimates. We saw that in our center-of-gravity plot.

That means estimating the degree of variability in the slope

```{r topgraphic-map-1, fig.asp = 1}
ggplot(df_pulled) + 
  aes(x = Intercept, y = Slope_Days, color = Model) + 
  # Draw contour lines from the distribution of 
  geom_path(aes(group = level, color = NULL), data = df_ellipse, 
            linetype = "dashed", color = "grey40") + 
  # Draw an arrow connecting the observations between models
  geom_path(aes(group = Subject, color = NULL), 
            arrow = arrow(length = unit(.02, "npc"))) + 
  # Layer the points next so they land on the arrowheards
  geom_point(data = df_gravity, size = 5) + 
  geom_point(size = 2) + 
  theme(legend.position = "bottom") + 
  ggtitle("Topographic map of regression parameters") + 
  xlab("Intercept estimate") + 
  ylab("Slope estimate") + 
  scale_color_brewer(palette = "Dark2") 
```

There are a few tweaks we might consider for this plot. I don't think the ovals
need to be contained in the plot. The points are more important, and the
plotting boundaries should be set with respect to the points. We can redefine
the limits by using `coord_cartesian()`.

```{r topographic-map-2, fig.asp = 1}
last_plot() +
  coord_cartesian(
    xlim = range(df_pulled$Intercept), 
    ylim = range(df_pulled$Slope_Days),
    expand = TRUE) 
```

To go all out :nerd:, let's also label the contours with the confidence levels.
I see that the lower left area is relatively free of points, so I can place the
labels there. I filter down to just the ellipse points in the bottom 25% of _x_
and _y_ values. That will keep points in that quadrant. Then I find the _x_-_y_
point with the farthest distance from the center as the location for my label.

```{r topographic-map-3, fig.asp = 1}
# Euclidean distance
contour_dist <- function(xs, ys, center_x, center_y) {
  x_diff <- (center_x - xs) ^ 2
  y_diff <- (center_y - ys) ^ 2
  sqrt(x_diff + y_diff)
}

df_label_locations <- df_ellipse %>% 
  group_by(level) %>%
  filter(Intercept < quantile(Intercept, .25), 
         Slope_Days < quantile(Slope_Days, .25)) %>% 
  mutate(dist = contour_dist(Intercept, Slope_Days, 
                             fixef(m)[1], fixef(m)[2])) %>% 
  top_n(-1, wt = dist) %>% 
  ungroup()

last_plot() +
  geom_text(aes(label = level, color = NULL), data = df_label_locations, 
            nudge_x = .5, nudge_y = .8, size = 4, color = "grey40")
```


## Plotting lines from a Bayesian mixed effects model

If we fit a Bayesian model, we can sample from a posterior distribution of
partially pooled regression lines. First, we fit the model in RStanARM with
weakly informative priors. 

```{r}
library(rstanarm)
```

```{r sleep-glmer-model, results='hide', cache = TRUE}
b <- stan_glmer(
  Reaction ~ Days + (Days | Subject),
  family = gaussian(),
  data = df_sleep,
  prior = normal(0, 2),
  prior_intercept = normal(0, 5),
  prior_covariance = decov(regularization = 2),
  prior_aux = cauchy(0, 1))
```

We get a similar overview as before.

```{r}
b
```

We have posterior distribution of values now! That means instead of one "center 
of gravity" point, we have 4,000 plausible points for our central value. (The
center of our former contour plot has its own contour plot. That's Bayes for 
you.)

```{r posterior-of-central-point}
# Get a dataframe: One row per posterior sample
df_posterior <- b %>% 
  as.data.frame() %>% 
  as_tibble()

ggplot(df_posterior) + 
  aes(x = `(Intercept)`, y = `Days`) + 
  stat_density_2d(aes(fill = ..level..), geom = "polygon") +
  ggtitle("Where's the average intercept and slope?") + 
  xlab("Estimate for average intercept") + 
  ylab("Estimate for average slope") +
  labs(fill = "Posterior\ndensity") + 
  coord_cartesian(
    xlim = range(df_pulled$Intercept), 
    ylim = range(df_pulled$Slope_Days),
    expand = TRUE) 
```

For each participant, we have 4,000 partially-pooled regression lines too, so we
can visualize our uncertainty for each participant's individual regression line.

Let's finish by drawing a sample of those lines for a faceted plot. We have to
do a bunch of data wrangling to get a dataframe with one row per subject per 
posterior sample.

```{r}
# For each sample, add the average intercept and average slope values to each
# participant's deviation from that average. These yields the intercept and
# slope parameters for each participant.
df_effects <- df_posterior %>%
  # Find all the columns with the pattern "b[(Intercept". Add the column
  # df_posterior$`(Intercept)` to each of those columns.
  mutate_at(
    .vars = vars(matches("b\\[\\(Intercept")), 
    .funs = funs(. + df_posterior$`(Intercept)`)) %>%
  # Again for slope
  mutate_at(
    .vars = vars(matches("b\\[Day")), 
    .funs = funs(. + df_posterior$Days))

# Convert to a long format
df_long_effects <- df_effects %>%
  select(matches("b\\[")) %>%
  tibble::rowid_to_column("draw") %>%
  tidyr::gather(Parameter, Value, -draw)

# Extract the effect type and subject number from each parameter name
df_long_effects$Type <- df_long_effects$Parameter %>%
  stringr::str_detect("Intercept") %>%
  ifelse(., "Intercept", "Slope_Day")

df_long_effects$Subject <- df_long_effects$Parameter %>%
  stringr::str_extract("\\d\\d\\d")

df_long_effects <- df_long_effects %>% 
  select(draw, Subject, Effect = Type, Value)

# Finally!
df_long_effects
```

Now that we have the data in the right shape, we are going randomly choose 50
posterior samples and plot those lines alongside the observed data.

```{r posterior-of-indvidual-lines, fig.asp = .9}
df_samples <- df_long_effects %>%
  filter(draw %in% sample(1:4000, size = 50)) %>%
  tidyr::spread(Effect, Value)
df_samples

ggplot(df_sleep) +
  aes(x = Days, y = Reaction) +
  geom_abline(aes(intercept = Intercept, slope = Slope_Day), 
              data = df_samples, color = "#3366FF", alpha = .1) +
  geom_point() +
  facet_wrap("Subject") + 
  labs(x = xlab, y = ylab) 
```

For the participants with complete data, the lines pile up and form a narrow 
band, indicating a low degree of uncertainty. In the final two panels, however,
we only have limited data, and the sample of lines fan out and cover many
different plausible trajectories.

The uncertainty is more dramatic if we plot a contour plot for each
participant---basically, drawing each participants' mostly likely locations in
the landscape of parameter values.

```{r posterior-of-indvidual-parameters, fig.asp = .8}
ggplot(df_long_effects %>%  tidyr::spread(Effect, Value)) + 
  aes(x = Intercept, y = Slope_Day) + 
  stat_density_2d(aes(fill = ..level..), geom = "polygon") +
  facet_wrap("Subject") + 
  xlab("Intercept estimate") + 
  ylab("Slope estimate") +
  labs(fill = "Posterior\ndensity")
```

For 373 and 374, the contour regions are very tall: A lot of slope values are
plausible. The region for 374 is more off center and slightly narrow than that
of 373: That extra data point matters.

***

I hope this write-up helps students and users under mixed-effects models at a
more intuitive level,
I had formally learned about these models twice in graduate school. In 
psychology, we were told to use them if we wanted to make inferences about a 
larger population of subjects or stimulus items. In educational psychology, we 
were told to use them to capture the sources of variances in a nested data-set:
Kids nested in classrooms nested in schools, etc.
It wasn't until I taught myself Bayesian stats that I learned about third reason
to use them: They pool information across different units, providing regularized
model estimates. I find this rationale most intuitive. The [Gelman and Hill
book](http://amzn.to/2rVRZmw) and [_Statistical
Rethinking_](http://amzn.to/2ty0C3T) both discuss the partial pooling
description of these models. (Ooooh, as I added the _Rethinking_ link, I just
noticed that I created a ggplot2 version of the plot from the cover of that
book. :sunglasses:)



