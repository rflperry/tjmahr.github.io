---
title: Draft post (2020-08-05)
excerpt: ''
tags: ''
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
knitr::opts_chunk$set(eval = TRUE)
```

*update date when published*

For a long time, I've been curious about something. It is a truth casually
mentioned in textbooks, package documentation, and tweets: random effects and
smoothing splines are the same thing.

```{asis}
<blockquote class="twitter-tweet" data-conversation="none" data-dnt="true"><p lang="en" dir="ltr">random effects and splines are _the same_ thing. See also <a href="https://t.co/LgZTzZimH0">https://t.co/LgZTzZimH0</a></p>&mdash; DavidLawrenceMiller (@millerdl) <a href="https://twitter.com/millerdl/status/846719376338407424?ref_src=twsrc%5Etfw">March 28, 2017</a></blockquote> 
```

The short version is that these equations are the same:


This is from MGCV p.77 but with theta subscripts removed.

$$
\mathbf{y} = \mathbf{X\beta} + \mathbf{Zb} + \mathbf{\epsilon} \\
\mathbf{b} \sim \textsf{N}(\mathbf{0}, \mathbf{\psi}) \\
\mathbf{\epsilon} \sim \textsf{N}(\mathbf{0}, \mathbf{\Lambda}) \\
\mathbf{X}: \textrm{fixed effects model matrix} \\
\mathbf{Z}: \textrm{random effects model matrix} \\
\mathbf{\psi}, \mathbf{\Lambda} : \textrm{error structures} \\
$$

MGCV book p.174

$$
\mathbf{y} = \mathbf{X}^*\mathbf{\beta}^* + \mathbf{Zb} + \mathbf{\epsilon} \\
\mathbf{b} \sim \textsf{N}(\mathbf{0}, \mathbf{I}\sigma^2/\lambda) \\
\mathbf{\epsilon} \sim \textsf{N}(\mathbf{0}, \mathbf{I}\sigma^2) \\
\mathbf{X}: \textrm{fixed effects model matrix} \\
\mathbf{Z}: \textrm{random effects model matrix} \\
\mathbf{\psi}, \mathbf{\Lambda} : \textrm{error structures} \\
$$

## Mixed model review

Let's review what these things means. Mixed effects models, apparently the main
focus of this blog over the years, are used to estimate random or varying
effects.



Here is an example from Gelman and Hill (2007). Radon measurements were
taken in Minnesota counties. We would like to estimate the average radon
measurement for each county. We have a repeated measures situation. Some
counties have more observations than others. We use a mixed effects
model to estimate a population distribution of county estimates. These
county level estimates are randomly varying effects.


```{r}
library(tidyverse)
library(lme4)
radon <- rstanarm::radon

m <- lme4::lmer(log_radon ~ 1 + (1 | county), radon)
radon_aug <- broom.mixed::augment(m, radon) 

radon_aug <- radon_aug %>% 
  group_by(county) %>% 
  mutate(n = n(), mean = mean(log_radon)) %>% 
  ungroup()

# lattice::dotplot(ranef(m))

okay <- radon_aug$county %>% 
  fct_infreq() %>% 
  fct_count() 

# create_thresholder <- function(limit) {
#   function(x) {
#     x > limit
#   }
# }
# 
# over_100 <- Position(create_thresholder(100), okay$n, right = TRUE)
# over_50 <- Position(create_thresholder(50), okay$n, right = TRUE)
# over_25 <- Position(create_thresholder(25), okay$n, right = TRUE)
# over_10 <- Position(create_thresholder(10), okay$n, right = TRUE)
# over_5 <- Position(create_thresholder(5), okay$n, right = TRUE)


p0 <- ggplot(radon_aug) + 
  aes(x = fct_infreq(county)) + 
  geom_bar() + 
  labs(x = NULL, y = "n") + 
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank()
  ) 
  

p1 <- ggplot(radon_aug) + 
  aes(x = fct_infreq(county), y = log_radon) + 
  stat_summary(
    fun.data = mean_se,
    color = "grey50", 
    fatten = 2
  ) + 
  geom_point(aes(y = .fitted), color = "blue") + 
  labs(x = "county", y = "log(radon)") +
  geom_hline(
    yintercept = fixef(m)[1]
  ) +
  annotate(
    "text", x = 6, label = "mean +- se", y = 2.15, color = "grey50", hjust = 0
  ) +
  annotate(
    "text", x = 4, label = "mixed model\nestimate", y = 1.7, color = "blue", hjust = 0
  ) +
  annotate(
    "text", x = 10, label = "population\nmean\nestimate", y = 1.5, color = "black", hjust = 0
  ) +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank()
  ) 
library(patchwork)
p0 + p1 + plot_layout(ncol = 1, heights = c(1, 4))  

vars <- broom.mixed::tidy(m, effects = "ran_pars", scales = "vcov")
var_g <- vars$estimate[1]
var_y <- vars$estimate[2]

# another try: variance ratios

# icc
var_g / (var_y + var_g)

# between group information is like this many observations within a county
var_y / var_g

# I wanted to add funneling lines to show the effect but messed up
# 
# radon_aug <- radon_aug %>% 
#   mutate(
#     diff = mean - mean(log_radon),
#     diff2 = mean - .fixed,
#     # page 733 in CAR
#     denom = 1 + ((var_y) / (n * var_g)),
#     # page 76 in Davidian
#     blup = mean - ((var_y) / (n * var_g + var_y)) * (mean - .fixed),
#     blup1.8 = 1.8 - ((var_y) / (n * var_g + var_y)) * (1.8 - .fixed)
#   )
# 
# sum(radon_aug$.hat)
# # sum(gam$edf)
# 
# ggplot(radon_aug) + 
#   aes(x = n, y = diff2) + 
#   geom_point() + 
#   geom_line(aes(y = 1.8 - (1.8) / denom))

# ggplot(radon_aug) + 
#   aes(x = n, y = mean - .fitted) + 
#   geom_point() + 
#   geom_point(aes(y = .fitted - .fixed), color = "red") + 
#   geom_point(aes(y = blup - .fixed), color = "pink") + 
#   geom_line(aes(y = blup1.8))
# 
# Probably should replace mean with regression estimate.
# ggplot(radon_aug) + 
#   aes(x = n, y = mean - .fixed) + 
#   geom_point() +
#   scale_x_log10()

# I want to know the effective degrees of freedom.
# Wood p.83
# Paraphrase of book: suppose b ~ N(0, sigmab). How many dfs with b? If sigmab
# 0, then b does nothing. If sigmab is bigger and bigger, df = p groups, then we
# have a fixed effects model. "This suggests that the effective degrees of
# freedom for b should increase with sigma_b, from 0 up to p." 

# ?summary.gam() says that edf comes from the trace of the influence matrix.

icc <- var_g / (var_y + var_g)

var_y / var_g
deff = var_y / var_g

ns <- table(radon$county)
sum(ns / (1 + (ns - 1) * icc))

deff = 1 * (nobs(m) - 1) * icc
nobs(m) / deff

```


The first figure illustrates the observed county means and the estimated ones.
We see a classic example of partial pooling. For counties with many
observations, the estimate mean is hardly adjusted. For counties with less data,
the estimate is pulled towards the group mean. 

The contention behind the smooths = random effects claim is that what we just
did is a case of *smoothing*.


```{asis}
<blockquote class="twitter-tweet" data-conversation="none" data-lang="en" data-dnt="true" data-theme="light">
  <p lang="en" dir="ltr">Sadly, I feel like my career has peaked with the creation of this meme <a href="https://t.co/5ilRFonsy7">pic.twitter.com/5ilRFonsy7</a></p>

  <img src="/assets/images/spider-smooth.jpg" alt="Spiderman (Penalized smooths) pointing at (and being pointed at) by Spiderman (Random effects)" />
  
  &mdash; Eric Pedersen (@ericJpedersen) <a href="https://twitter.com/ericJpedersen/status/1293508069016637440?ref_src=twsrc%5Etfw">August 12, 2020</a>
</blockquote> 
```


### First let's make a helper function for plotting matrices

Base R includes a nice function for plotting each column of a matrix, so that e.g., we can draw a line for each column of the matrix. In this example, `poly()` creates a column for each polynomial term and `matplot()` plots a line for each one.

```{r}
xs <- sort(rnorm(100))
polynomials <- cbind(xs, polynomials)
x <- polynomials
focal_column <- 1
head(polynomials)

polynomials <- poly(xs, 3)
matplot(polynomials, type = "l")


```

We are going to recreate this in ggplot2 for convenience.


```{r}
# ggplot2 version of matplot(x, type = "l")
ggmatplot <- function(x, focal_column = NULL, n_colors = 6) {
  # reshape into a long dataframe
  ux <- unique(x)

  if (!is.null(focal_column)) {
    focal_label <- rlang::expr_label(substitute(x[, focal_column]))
  } else {
    ux <- cbind(seq_len(nrow(ux)), ux)
    focal_column <- 1
    focal_label <- "unique row number"
  }
  
  rownames(ux) <- seq_len(nrow(ux))
  # colnames(ux) <- seq_len(nrow(ux))

  long_ux <- reshape2::melt(
    ux[, -focal_column, drop = FALSE], 
    c(".row", ".column")
  )
  
  focal <- reshape2::melt(
    ux[, focal_column, drop = FALSE], 
    c(".row", ".focal_name"), 
    value.name = focal_label
  )
  long_ux <- merge(long_ux, focal)
  
  
  # cycle through colors like matplot()
  column_numbers <- match(long_ux$.column, unique(long_ux$.column))
  long_ux$.color_cycle <- factor(column_numbers %% n_colors)
  ggplot(long_ux) + 
    # cycle the colors
    aes(x = !! rlang::sym(focal_label), y = value, color = .color_cycle) + 
    geom_line(
    # aes(group = !! rlang::sym(focal_label)),
      aes(group = .column)
    ) + 
    guides(color = FALSE) +
    scale_color_manual(
      values = unname(palette.colors(n_colors, palette = "R4"))
    ) +
    labs(title = rlang::expr_label(substitute(x)))
}


ggmatplot(polynomials)

ggmatplot(cbind(xs, polynomials), 1)


```



### Smoothing example

Let's do a smooth with a generalized additive model. Here we use the
`mcycle` dataset which gives the head acceleration in a simiulated
motorcycle accident. Here are going to fit a model and plot the smooth from it. And then we are going to work through what the model did.



```{r}
library(mgcv)
select <- dplyr::select

mcycle <- MASS::mcycle %>% 
  tibble::rowid_to_column()

mcycle_smooth <- gam(
  accel ~ 1 + s(times, bs = "ps", k = 15), 
  data = mcycle, 
  method = "ML"
)

mcycle$.fitted <- fitted(mcycle_smooth)

ggplot(mcycle) + 
  aes(x = times, y = accel) + 
  geom_point() + 
  geom_line(
    aes(y = .fitted), 
    color = "blue"
  )

```


So what happened here? We start with some *splines*. These splines are a bunch of
wiggly lines that are weighted and summed together to approximate some nonlinear
function. We are decomposing the `times` predictor into a bunch of individual
sub-trends that are weighted and summed together. My post on orthogonal
polynomial illustrates the same principle but with polynomial trends.

- [ ] link post


An easy way to pull the wiggles is to use the model matrix. 

<!-- The bumps here are a little pointy because the *x* values of line of the match -->
<!-- the data. -->

```{r}
annotate_grey <- function(label, x, y, size = 4, ...) {
  annotate(
    "label", x = x, y = y, label = label, size = size,
    hjust = 0, vjust = 0, fill = scales::alpha("grey93", .6), 
    label.size = 0, ...
  )
}

m <- cbind(mcycle$times, model.matrix(mcycle_smooth))

ggmatplot(model.matrix(mcycle_smooth)) +
  annotate_grey("intercept", 5, .9, size = 5) +
  annotate_grey("splines at x values", 60, .66, size = 5)

ggmatplot(m, 1) +
  annotate_grey("intercept", 5, .9, size = 5) +
  annotate_grey("splines at x values", 40, .66, size = 5)
```

Now we can weight these by multiplying by the model coefficients. Here we use
the `diag(coef())` trick to prevent the weighted predictors from being summed together.


```{r}
weighted_coefs <- model.matrix(mcycle_smooth) %*% diag(coef(mcycle_smooth))

ggmatplot(weighted_coefs) +
  annotate_grey("intercept", 5, -36, size = 5) +
  annotate_grey("weighted splines at x values", 5, 26, size = 5)

ggmatplot(weighted_coefs, n_colors = 1) + 
  annotate_grey("sum", 20, -70, size = 5, color = "maroon") + 
  annotate_grey("weighted splines", 10, 25, size = 5, ) +
  annotate_grey("intercept", 60, -25, size = 5, )
```

If we sum the lines together, we get the regression line.

```{r}
ggmatplot(weighted_coefs, n_colors = 1) + 
  stat_summary(
    aes(group = 1), 
    color = "maroon", 
    fun = sum, 
    geom = "line", 
    size = 2
  ) +
  annotate_grey("sum", 20, -70, size = 5, color = "maroon")


mcycle$.fitted <- fitted(mcycle_smooth)
ggplot(mcycle) + 
  aes(x = times, y = accel) + 
  geom_point() +
  geom_line(aes(y = .fitted)) 

predict(mcycle_smooth)
```

What I have done so far is describe is regression with a basis function.
Smoothing splines go one step further: They penalize wiggliness to prevent
overfitting. The idea is as follows: We chose 15 knots for the last one. Where
did that number come from? What if we specified 30 knots? That's 30 predictors.
Isn't it really easy to overfit the data with this approach? 

Well, let's look at the 30-knot version.





### Take a step back

I have not touched this for a few months so I need to start taking some notes to refresh my memory.

I decided to look at p splines because they have a nice random walk formulation in the GAMLSS book. Let me summarize that.



"Random effect models deat with correlation between individual
observations by introducing different sources of variation in the data.
This is achieved by assumin that there are random effects variables γ
providing an additional source of variation.... [it talks about pdfs
and hyperparameters]" [p.322 GAMLSS book].




3.2

9.6

- they say that penalty matrix tends to be product of two difference matrices

```{r}
diff(diag(10), diff = 2)
d2 <- diff(diag(10), diff = 2)
penalty_matrix <- t(d2) %*% d2
ggmatplot(d2)
ggmatplot(penalty_matrix)

```

- they show that lambda is a ratio of variances in a random effects smoothing model

We can do this ourselves from a random effects model fitted with gam.

```{r}
# bs = "re" use a random effects basis.
gam_re <- gam(log_radon ~ s(county, bs = "re"), data = radon, method="ML")
gam_re
```

First we take ratio of the variances, based on the numbers from the
variance components. We can also pull out `sp` (smoothing parameter)
from the model. Indeed, the ratio of the variances is the same as
smoothing parameter.

```{r}
gam.vcomp(gam_re)
(0.7660732 ^ 2) /  (0.2939025 ^ 2)

gam_re$sp
```

```{r}
diag(10) %*% diag(10)
```


The random walks part from 268. The first case are random intercepts.

$$
\gamma_j \sim \mathcal{N}(0, \sigma^{2}_{b}) \\ 
\gamma_j - \gamma_{j-1} \sim \mathcal{N}(0, \sigma^{2}_{b}) \\ 
\gamma_j - 2\gamma_{j-1} - \gamma_{j-2} \sim \mathcal{N}(0, \sigma^{2}_{b})
$$



```{r}
diag(8)
diff(diag(8), diff = 1)
diff(diag(8), diff = 2)

ggmatplot(diag(gam_re$coefficients[-1]))
```

And there they are, walkin' around 0 randomly.

```{r}
diff_14 <- diff(diag(14), diff = 2)
# ggmatplot(diff_14)
# ggmatplot(diag(mcycle_smooth$coefficients[-1]))
ggmatplot(diag(as.vector(diff_14 %*% mcycle_smooth$coefficients[-1])))
```

And there they are, walkin' around 0 randomly.


### Old stuff, not sure about


```{r}
mcycle_smooth60 <- gam(
  accel ~ 1 + s(times, bs = "ps", k = 60), 
  data = mcycle, 
  method = "ML"
)


# X The model matrix from this term. This may have an "offset" attribute: a
# vector of length nrow(X) containing any contribution of the smooth to the
# model offset term. by variables do not need to be dealt with here, but if they
# are then an item by.done must be added to the object.
ggmatplot(x[[1]]$X)
#
# S A list of positive semi-definite penalty matrices that apply to this term.
# The list will be empty if the term is to be left un-penalized.
ggmatplot(x[[1]]$S[[1]])

#
# rank An array giving the ranks of the penalties.
x <- smoothCon(s(times, bs = "ps", k = 10), mcycle, scale.penalty = FALSE, absorb.cons = FALSE)
x
ggmatplot(x[[1]]$X )

str(x)
# 
# mcycle_smooth10 <- gam(
#   accel ~ 1 + s(times, bs = "ps", k = 60), 
#   data = mcycle, 
#   method = "ML",
#   sp = 10
# )
# 
# library(brms)
# mcycle_smooth60_b <- brm(
#   accel ~ 1 + s(times, bs = "ps", k = 60), 
#   data = mcycle,
#   backend = "cmdstanr"
# )
# 
# mcycle_smooth60_b
# 
# gam.vcomp(mcycle_smooth60, rescale = FALSE)
# 
# smoothers <- scales::log_breaks(10)(c(1, 1000000))
# coef(mcycle_smooth60)
# mcycle_smooth60$sp
# mcycle_smooth60$smooth[[1]]
# mcycle_smooth10$full.sp
# mcycle_smooth10$smooth[[1]][["S"]]
# 
# mcycle_smooth60$smooth[[1]][["S"]]
# 
# 
# mcycle_smooth60$cmX
# mcycle_smooth60$sp
# mcycle_smooth60$smooth[[1]][["S"]]
# mcycle_smooth60$smooth[[1]][["S.scale"]]
# # mcycle_smooth60$smooth[[1]][[sp]]
# 
# model_work <- tibble(sp = c(smoothers, mcycle_smooth60$sp)) %>% 
#   mutate(
#     models = sp %>% map(function(sp) update(mcycle_smooth60, sp = sp)),
#     .fitted = models %>% map(function(model) cbind(mcycle, .fit = fitted(model), .resid = residuals(model))),
#     .deriv2 = models %>% map(gratia::derivatives, order = 2, newdata = mcycle),
#     .penalty = models %>% map(
#       function(m) {
#         b <- coef(m)
#         k <- length(b)
#         # s <- m$smooth[[1]][["S"]][[1]]
#         d <- m$smooth[[1]][["D"]]
#         s <- crossprod(d)
#         as.vector(t(b) %*% s %*% b)
#       }
#     ),
#     sp2 = models %>% map_dbl(pluck, "smooth", 1, "sp")
#   ) %>% 
#   select(-models) %>% 
#   unnest(.penalty)
# 
# list(mcycle_smooth) %>% 
# str(models)
# 
# models <- model_work %>% 
#   select(-.deriv2) %>% 
#   unnest(.fitted)
# 
# deriv2 <- model_work %>% 
#   select(-.fitted) %>% 
#   unnest(.deriv2)
# 
# errors <- models %>% 
#   group_by(sp, sp2) %>% 
#   summarise(
#     .resid = sum(.resid ^ 2), 
#     .penalty = unique(.penalty),
#     .groups = "drop"
#   ) %>% 
#   mutate(
#     r = .resid + (1 / sp) * .penalty,
#     r2 = .resid + (1 / sqrt(sp)) * .penalty
#   )
# 
# m$aic
# 
# derivative <- deriv2 %>% 
#   group_by(sp) %>% 
#   summarise(.derivative = sum(derivative ^ 2))
# 
# errors %>% 
#   left_join(derivative) %>% 
#   mutate(
#     .resid + .derivative
#   )
# 
mcycle$.fitted60 <- fitted(mcycle_smooth60)

ggplot(mcycle) + 
  aes(x = times, y = accel) + 
  geom_point() + 
  # geom_line(aes(y = .fitted), color = "blue") + 
  geom_line(aes(y = .fitted60), color = "blue")
```

Huh, they hardly look any different. What's going on?

Penalties.


Behind the scenes, the model is

$$

$$



We are going to plot a lot of matrices to show what the splines are up to.

```{r}
d <- diff(diag(60), d = 2)
s <- crossprod(d)
dim(s)
s <- t(d) %*% d
b <- coef(mcycle_smooth60)

```



```{r, eval = FALSE}
library(mgcv)
x <- runif(30)
dat <- data.frame(x = x)
sm <- smoothCon(s(x, bs = "ps"), dat, absorb.cons	= FALSE)[[1]]
re <- smooth2random(sm,"",2)
str(re)
str(sm)
ggmatplot(sm$X)
matplot(re$Xf, type = "l")
matplot(sm$S[[1]], type = "l")
matplot(sm$D, type = "l")
matplot(re$trans.U, type = "l")
matplot(re$trans.D, type = "l")
matplot(re$pen.ind, type = "l")
```



```{r}
# tk <- seq(min(t),max(t),length=k) ## knotsX<-apply(diag(k),1,function(y) approx(tk,y,t,rule=2)$y)b <- lm(a ~ X-1)

# 

# 
# mcycle_smooth60
# mcycle_smooth60$smooth[[1]][["S"]]
# sum(mcycle_smooth60$edf2)
# sum(mcycle_smooth60$edf)
# 
# mcycle_smooth$sp
# mcycle_smooth60$sp
# 
# gam.vcomp(mcycle_smooth)
# gam.vcomp(mcycle_smooth60)
# X <- model.matrix(mcycle_smooth60)
# K <- 30
# D <- diff(diff(diag(K))) 
# ## t(D)%*%D is penalty coef matrix
# sp <- 2 ## square root smoothing parameter
# XD <- rbind(X, D * sp)  ## augmented model matrix
# y0 <- c(mcycle$accel,rep(0,nrow(D))) ## augmented data
# b <- lm(y0~XD-1)     
# ## fit augmented model
# plot(mcycle$times,mcycle$accel,ylab="accel",xlab="time")
# 
# lines(mcycle$times, X %*% coef(b),col=2,lwd=2)
```















```{r}
gratia::penalty(mcycle_smooth, rescale = TRUE) %>% 
  mutate(
    row = row %>% stringr::str_replace("^f(\\d)$", "f0\\1"),
    col = col %>% stringr::str_replace("^f(\\d)$", "f0\\1"),
  ) %>% 
gratia::draw()
```












Now we can look at the output from the mgcv summary.

```{r}
summary(mcycle_smooth)
gam.vcomp(mcycle_smooth)
```


The difference matrix

```{r}
diff(diag(5), diff = 1)
diag(5)

# note two rows disappear
d <- diff(diag(5), differences = 2)

# but now we have a five by five again
g <- t(d) %*% d


sum(t(d)[1, ] * d[, 1])
sum(t(d)[1, ] * d[, 2])
sum(t(d)[1, ] * d[, 3])

# crossprod(d)
# (t(d) %*% d) == crossprod(d)

matplot(d, type = "l")
matplot(g, type = "l")


```

Here is 


```{r}
library(mgcv)
radon <- rstanarm::radon
m <- gam(log_radon ~ 1 + s(county, bs = "re"), data = radon, method = "ML")
summary(m)

mf <- gam(log_radon ~ 1 + s(county, bs = "re", fx = TRUE), data = radon, method = "ML")
summary(mf)



ggmatplot(model.matrix(m) %*% diag(coef(m)), n_colors = 1) + 
  stat_summary(
    aes(group = 1), 
    color = "maroon", 
    fun = sum, 
    geom = "line", 
    size = 1
  ) +
  annotate_grey("sum", 9, .9, size = 5, color = "maroon") + 
  annotate_grey("\"weighted\" random effects", 10, .4, size = 5) +
  annotate_grey("intercept", 80, 1.1, size = 5) +
  coord_cartesian(xlim = c(0, 92))

# tjmisc::ggpreview()
re <- m$smooth[[1]]
ggmatplot(model.matrix(m))
# ggmatplot(re$S[[1]])

coef(m)
coef(mf)


ggmatplot(model.matrix(mcycle_smooth) %*% diag(coef(mcycle_smooth))) + 
  stat_summary(fun = sum, color = "red", geom = "line")

ggmatplot(model.matrix(m) %*% diag(coef(m)), n_colors = 1) + 
  stat_summary(fun = sum, color = "red", geom = "line")

ggmatplot(model.matrix(m) %*% diag(coef(m))) + stat_summary(fun = sum, color = "red", geom = "line")

ggmatplot(model.matrix(mf) %*% diag(coef(mf)))  + stat_summary(fun = sum, color = "red", geom = "line")

# mgcv::gam.vcomp(m)

m <- gam(log_radon ~ 1 + s(county, bs = "re"), data = radon, method = "ML", fit = FALSE)
# str(m)
```

(m is derivative penalized)



```{r, eval = FALSE}
# trying to write a naive stan model to do this.
matplotl <- function(...) matplot(..., type = "l")
z <- model.matrix(mcycle_smooth60)[, -1]
matplotl(z)

k <- ncol(z)
d <- diff(diag(k), differences = 2)
aug_d <- rbind(0, 0, d)
diag(aug_d) <- 1

X <- t(backsolve(t(aug_d), t(z)))
matplotl(aug_d)
matplotl(X)

ncol(X)
x1 <- matrix(rep(X[, c(1)], 59), ncol = 59)
x2 <- matrix(rep(X[, c(2)], 59), ncol = 59)
matplotl(X)
matplotl(X[, -c(1, 2)])
matplotl(X[, c(1, 2)])
matplotl(X + x1 + x2)
matplotl((X + x1 + x2)[, -1])
matplotl((X + x1)[, c(-1, -2)])
matplotl((X + x2)[, c(-1, -2)])




g <- crossprod()
matplotl(g)





n_fixed <- 1
n_knots <- 59
mcycle_smooth60_b

# Yes diagonal penalty
#" If TRUE then the smooth is reparameterized to turn the penalty into an
# identity matrix, with the final diagonal elements zeroed (corresponding to the
# penalty nullspace). May result in a matrix diagRP in the returned object for
# use by PredictMat."
sm <- smoothCon(s(times, bs = "cr", k = 60), mcycle, absorb.cons = TRUE, modCon = 3, diagonal.penalty = TRUE)[[1]]

ggmatplot(sm$X)
ggmatplot(sm$diagRP)
ggmatplot(sm$S[[1]])
ggmatplot(solve(t(sm$diagRP)))

# No diagonal penalty
sm0 <- smoothCon(s(times, bs = "cr", k = 60), mcycle, absorb.cons = TRUE, modCon = 3, diagonal.penalty = FALSE)[[1]]


ggmatplot(sm0$X)
ggmatplot(sm0$S[[1]])
ggmatplot(sm0$X %*% sm0$S[[1]])

X0 <- sm0$X
K <- sm0$rank
# K <- 10
aug_d <- rbind(0, 0, diff(diag(K), difference = 2))
diag(aug_d) <- 1
X <- t(backsolve(t(aug_d), t(X0)))
ggmatplot(sm0$X)
ggmatplot(aug_d)
ggmatplot(X[-c(1, 2)])


# # No diagonal penalty
# sm0 <- smoothCon(s(times, bs = "cr", k = 60), mcycle, absorb.cons = FALSE, modCon = 3, diagonal.penalty = FALSE)[[1]]
# 
# ggmatplot(sm0$X)
# ggmatplot(sm0$S[[1]])
# ggmatplot(sm0$X %*% sm0$S[[1]])
# 
# X0 <- sm0$X
# K <- sm0$rank
# # K <- 10
# aug_d <- rbind(0, 0, diff(diag(K), difference = 2))
# diag(aug_d) <- 1
# X <- t(backsolve(t(aug_d), t(X0)))
# matplotl(sm0$X)
# matplotl(aug_d)
# matplotl(X[c(1, 2)])
# matplotl(X[, 1] + X[, 2] + X[, 3])
# matplotl(X[, 50])




# Should be the penalty matrix
ggmatplot(crossprod(diff(diag(59), differences = 2)))
# matplot(crossprod(diff(diag(59), differences = 2)), type = "l")

dim(sm0$X)
dim(sm0$S[[1]])















# looking at what standata is doing
sd <- brms::make_standata(accel ~ 1 + s(times, bs = "ps", k = 60), data = mcycle)
ggmatplot(sd$Zs_1_1)

ggmatplot(matrix(rep(sd$Xs, sd$knots_1), ncol = sd$knots_1) + sd$Zs_1_1)

str(sm$S)
str(sm)

mgcv:::smooth2random.mgcv.smooth

re <- smooth2random(sm, "", type = 2)
head(re$Xf) 
head(sd$Xs)



X0 <- sd$Zs_1_1
K <- sd$knots_1
# K <- 10
aug_d <- rbind(0, 0, diff(diag(K), difference = 2))
diag(aug_d) <- 1
X <- t(backsolve(t(aug_d), t(X0)))

ggmatplot(X0)

ggmatplot(X)

# K <- 40        
# ## basis dimension
# t0 <- min(mcycle$times);
# t1 <- max(mcycle$times)
# tk=seq(t0,t1,length=K)        
# ## knot sequence
# X <- tf.X(x=mcycle$times,xk=tk) 
# ## model matrix
# b <- coef(lm(mcycle$accel~X-1)) 
# ## fit model
# Xp <- tf.X(x=0:120/2,xk=tk)     
# ## prediction matrix
# plot(mcycle$times,mcycle$accel,ylab="accel",xlab="time")
# lines(0:120/2,Xp%*%b,col=2,lwd=2)

X0 <- sm$X
K <- 59
aug_d <- rbind(0, 0, diff(diag(K), difference = 2))
diag(aug_d) <- 1

X <- t(backsolve(t(aug_d), t(X0)))
ggmatplot(X0)
ggmatplot(aug_d)
gmatplot(X)
Z <- X[, -c(1,2)]
X <- X[, c(1:2)]
matplot(X)
matplot(Z)



sm_no_diag <- smoothCon(
  s(times, bs = "cr", k = 60), 
  mcycle, 
  absorb.cons = TRUE, 
  modCon = 3, 
  diagonal.penalty = FALSE)[[1]]

ggmatplot(sm_no_diag$X)
ggmatplot(sm_no_diag$S[[1]])

sm <- smoothCon(
  s(times, bs = "cr", k = 60), 
  mcycle, 
  absorb.cons = TRUE, 
  modCon = 3, 
  diagonal.penalty = TRUE)[[1]]

ggmatplot(sm$X)
ggmatplot(sm$diagRP)

ggmatplot(sm$X %*% diag(sm$diagRP))












data <- list(
  N = length(mcycle_smooth60$y),
  y = mcycle_smooth60$y,
  n_fixed = 2,
  n_knots = ncol(re$rand$Xr),
  spline_matrix = re$rand$Xr, 
  fixed_matrix = cbind(1, re$Xf)
)

m <- "
data {
  int<lower=0> N;               
  vector[N] y;                     
  int<lower=1> n_fixed;         
  int<lower=1> n_knots;         
  matrix[N,n_knots] spline_matrix;
  matrix[N,n_fixed] fixed_matrix;
}
parameters {
  real<lower=0> sigma_y;
  real<lower=0> sigma_knots;
  vector[n_fixed] beta_fixed;
  vector[n_knots] beta_knots;
}
model {
  sigma_y ~ normal(0, 100);
  beta_fixed ~ normal(0, 50);
  
  sigma_knots ~ normal(0, 100);
  beta_knots ~ normal(0, sigma_knots);

  y ~ normal(fixed_matrix * beta_fixed + spline_matrix * beta_knots, sigma_y);
}
"

# this doesn't seem to do any penalization. need to incorporate the penalty matrix?
# but how to do i get that? how do i incorporate it?

library(cmdstanr)
spline_mod <- cmdstan_model(write_stan_tempfile(m))

spline_fit <- spline_mod$sample(
  data = data,
  seed = 123,
  chains = 4,
  parallel_chains = 2,
  refresh = 500
)

spline_fit
gam.vcomp(mcycle_smooth60, rescale = FALSE)



# mcycle_smooth60_b$model
# mcycle_smooth60_b$stanvars
# mcycle_smooth60_b$data
# mcycle_smooth60_b$fit@inits
# mcycle_smooth60_b$fit@stanmodel

library(mgcv)
x <- runif(30)


sm <- smoothCon(s(times, bs = "cr", k = 8), mcycle, absorb.cons = TRUE, modCon = 3)[[1]]
matplot(sm$X, type = "l")

sm2 <- smoothCon(s(times, bs = "cr", k = 8), mcycle, absorb.cons = TRUE, modCon = 3, diagonal.penalty = TRUE)[[1]]
matplot(sm2$X, type = "l")

sm3 <- smooth2random(sm2, "", type = 2)
matplot(sm3$rand[[1]], type = "l")
matplot(sm3$rand, type = "l")

# this seems like a good lead but it feels like a cheat?
str(re)
# sm %*%

matplot(sm$X, type = "l")
matplot(sm$S[[1]], type = "l")
matplot(re$rand$Xr, type = "l")

matplot(sm$S[[1]], type = "l")


# matplot(re$rand$Xr$X)
# brms:::tidy_smef(mcycle_smooth60_b$formula, mcycle_smooth60_b$data)

# brms:::standata_basis(mcycle_smooth60_b)

# data_predictor(mcycle_smooth60_b$formula)
# 
# # where does brms find its data?
# brms::standata(mcycle_smooth60_b)
# model.matrix(mcycle_smooth60)
# 
# try <- rstan::read_stan_csv(spline_fit$output_files())
# l <- loo(try)
# 
# summary(mcycle_smooth60)
# gam.vcomp(mcycle_smooth60, rescale = FALSE)
```






```{r}
sm <- smoothCon(s(times, bs = "ps", k = 60), mcycle, absorb.cons = TRUE, modCon = 3)[[1]]
ggmatplot(sm$X)
ggmatplot(model.matrix(mcycle_smooth60))
dim(model.matrix(mcycle_smooth60))
dim(sm$X)
ggmatplot(sm$S[[1]])
range(MASS::mcycle$times)
ggmatplot(PredictMat(sm, data.frame(times = seq(2, 58, length.out = 1000))))


# Wood, p. 174, section 4.2.4
x <- sm$X
create_difference_matrix <- function(knots) {
  diff(diag(knots), differences = 2)
}
create_augmented_difference_matrix <- function(knots) {
  d <- rbind(0, 0, diff(diag(knots), differences = 2))
  diag(d) <- 1
  d
}
create_mixed_matrices <- function(x, knots = NULL) {
  if (is.null(knots)) { 
    knots <- ncol(x)
  }
  d <- create_augmented_difference_matrix(knots)
  x2 <- t(backsolve(t(d), t(x)))
  x2
}

ggmatplot(x)
d <- create_augmented_difference_matrix(ncol(x))
ggmatplot(d)

d0 <- create_difference_matrix(ncol(x))

x2 <- create_mixed_matrices(x)
ggmatplot(x2)
ggmatplot(x2[,-c(1,2)])
ggmatplot(x2[,c(1,2)])

re1 <- smooth2random(sm, "", type = 1)

x <- sm$X

data <- list(
  N = length(mcycle_smooth60$y),
  y = mcycle_smooth60$y,
  n_fixed = 2,
  n_knots = ncol(sm$X),
  spline_matrix = sm$X, 
  fixed_matrix = model.matrix(~ 1 + times, data = mcycle)
)

data2 <- list(
  N = length(mcycle_smooth60$y),
  y = mcycle_smooth60$y,
  n_fixed = 2,
  n_knots = ncol(x2[,-c(1,2)]),
  spline_matrix = x2[,-c(1,2)], 
  fixed_matrix = x2[,c(1,2)]
)
```

```
// random walk version
data {
  int<lower=0> N;               
  vector[N] y;                     
  int<lower=1> n_fixed;         
  int<lower=1> n_knots;         
  matrix[N,n_knots] spline_matrix;
  matrix[N,n_fixed] fixed_matrix;
}
parameters {
  real<lower=0> sigma_y;
  real<lower=0> sigma_knots;
  vector[n_fixed] beta_fixed;
  vector[n_knots] gamma_knots;
}
transformed parameters {
  vector[n_knots] beta_knots;
  // beta_knots[1] = gamma_knots[1];
  // beta_knots[2] = gamma_knots[2] - 2 * gamma_knots[1];
  for (i in 3:n_knots) {
    beta_knots[i] = gamma_knots[i] - 2 * gamma_knots[i-1] - gamma_knots[i-2];
  }
  
}

model {
  sigma_y ~ normal(0, 100);
  beta_fixed ~ normal(0, 50);
  
  sigma_knots ~ normal(0, 100);
  beta_knots ~ normal(0, sigma_knots);

  y ~ normal(fixed_matrix * beta_fixed + spline_matrix * beta_knots, sigma_y);
}
```

```{r}
# The random walk is for p-splines
m_rw <- "
data {
  int<lower=0> N;               
  vector[N] y;                     
  int<lower=1> n_fixed;         
  int<lower=1> n_knots;         
  matrix[N,n_knots] spline_matrix;
  matrix[N,n_fixed] fixed_matrix;
}
transformed data {
//  matrix[n_knots, n_knots] d; 
//  d = add_diag(d, 1);
  
}
parameters {
  real<lower=0> sigma_y;
  real<lower=0> sigma_knots;
  vector[n_fixed] beta_fixed;
  vector[n_knots] raw_knots;
}
transformed parameters {
  vector[n_knots] beta_knots;
  real sd_knots;
  beta_knots[1] = raw_knots[1];
  beta_knots[2] = raw_knots[2];
  for (i in 3:n_knots) {
    beta_knots[i] = raw_knots[i] - 2 * beta_knots[i-1] - beta_knots[i-2];
  }
  sd_knots = sd(beta_knots);
  
}

model {
  sigma_y ~ normal(0, 100);
  beta_fixed ~ normal(0, 50);
  
  sigma_knots ~ normal(0, 100);
  // beta_knots ~ normal(0, sigma_knots);
  raw_knots ~ normal(0, sigma_knots);

  y ~ normal(fixed_matrix * beta_fixed + spline_matrix * beta_knots, sigma_y);
}
"

m_no_diff <- "
data {
  int<lower=0> N;               
  vector[N] y;                     
  int<lower=1> n_fixed;         
  int<lower=1> n_knots;         
  matrix[N,n_knots] spline_matrix;
  matrix[N,n_fixed] fixed_matrix;
}
parameters {
  real<lower=0> sigma_y;
  real<lower=0> sigma_knots;
  vector[n_fixed] beta_fixed;
  vector[n_knots] beta_knots;
}
model {
  sigma_y ~ normal(0, 100);
  beta_fixed ~ normal(0, 50);
  
  sigma_knots ~ normal(0, 100);
  beta_knots ~ normal(0, sigma_knots);
  
  y ~ normal(fixed_matrix * beta_fixed + spline_matrix * beta_knots, sigma_y);
}
"
```

```{r}
library(cmdstanr)
spline_mod <- cmdstan_model(write_stan_tempfile(m_rw))

spline_fit <- spline_mod$sample(
  data = data,
  seed = 123,
  chains = 4,
  parallel_chains = 2,
  refresh = 500
)

spline_fit$summary("sd_knots")

spline_mixed_mod <- cmdstan_model(write_stan_tempfile(m_no_diff))

spline_mixed_fit <- spline_mod$sample(
  data = data2,
  seed = 123,
  chains = 4,
  parallel_chains = 2,
  refresh = 500
)
spline_mixed_fit


```



















```{r}

m_big <- "
data {
  int<lower=0> N;               // n data
  int<lower=0> y[N];            // data
  int<lower=1> L;               // groups
  int<lower=1,upper=L> ll[N];   // group membership
}
transformed data {
  real group_counts[L];

  group_counts = rep_array(0.0, L);

  for (i in 1:N) {
    group_counts[ll[i]] += ll[i];
  }

  real hm = mean(group_counts);

}
parameters {
  real<lower=0> sigma_y;
  real<lower=0> sigma_l;
  real grand_mean;
}
transformed parameters {
  real var_between = square(sigma_l);
  real var_within = square(sigma_y);
  real var_total = var_within + var_between;
}
model {
  matrix[N,N] bigma;
  vector[N] e;
  vector[N] zeroes;

  zeroes = rep_vector(0.0, N);
  bigma = rep_matrix(0.0, N, N);

  sigma_l ~ normal(0, 50);
  sigma_y ~ normal(0, 50);
  grand_mean ~ normal(0, 100);

  for (i in 1:N) {
    for (j in 1:N) {
      if (ll[i] == ll[j]) {
        if (i == j) {
          bigma[i,j] = var_total;
        } else {
          // If you divide everything by var_total this becomes
          // var_between / var_total which is the proportion of
          // of variance explained by between-group variance which
          // is the ICC
          bigma[i,j] = var_between;
        }
      }
    }
  }

  e = to_vector(y) - grand_mean;
  e ~ multi_normal(zeroes, bigma);
}
generated quantities {
  // how can we recover the group means?
  real<lower = 0, upper = 1> icc;
  icc = var_between / var_total;
  real mean_1 = mean((to_vector(y) - grand_mean)[1:3]);
  real mean_2 = mean((to_vector(y) - grand_mean)[4:6]);

  // trying to do effective sample size
  real deff = 1 + (hm - 1) * icc;
}
"

```

SVD thread https://twitter.com/WomenInStat/status/1285610321747611653?s=20
\@daniela_witten



```{r}
# object <- ~ s(times, bs = "cr", k = 60)
# mcycle <- MASS::mcycle
# knots=NULL
# 
# smooth.construct.cr.smooth.spec
# 
# sm <- smoothCon(
#   s(times, bs = "cr", k = 60), 
#   mcycle, absorb.cons = TRUE, modCon = 0)
# 
# q <- mgcv:::nat.param(sm$X, sm$S[[1]], type=0, unit.fnorm=FALSE)
# ggmatplot(q$X)
# ggmatplot(matrix(q$D))
# ggmatplot(q$P)

```

```{r}
# gamlss::cs
# gamlss::gamlss.cs(mcycle$times, mcycle$accel, w = rep(1, 133), df = 10)
# scs <- gamlss::scs
# g <- gamlss::gamlss(accel ~ scs(times), data = mcycle)
# summary(g)
# gamlss::getSmo(g)
# scs

```

