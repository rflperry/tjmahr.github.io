---
title: Draft post (2020-08-05)
excerpt: ''
tags: ''
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

*update date when published*

For a long time, I've been curious about something. It is a truth casually
mentioned in textbooks, package documentation, and tweets: random effects and
smoothing splines are the same thing.

```{asis}
<blockquote class="twitter-tweet" data-conversation="none" data-dnt="true"><p lang="en" dir="ltr">random effects and splines are _the same_ thing. See also <a href="https://t.co/LgZTzZimH0">https://t.co/LgZTzZimH0</a></p>&mdash; DavidLawrenceMiller (@millerdl) <a href="https://twitter.com/millerdl/status/846719376338407424?ref_src=twsrc%5Etfw">March 28, 2017</a></blockquote> 
```



## Mixed model review

Let's review what these things means. Mixed effects models, apparently the main
focus of this blog over the years, are used to estimate random or varying
effects.



Here is an example from Gelman and Hill (2007). Radon measurements were
taking in Minnesota counties. We would like to estimate the average radon
measurement for county. Some counties have more observations than others. We
have a repeated measures situation. We use a mixed effects model to estimate a
population distribution of county estimates. These county level estimates are
randomly varying effects.


```{r}
library(tidyverse)
library(lme4)
radon <- rstanarm::radon

m <- lme4::lmer(log_radon ~ 1 + (1 | county), radon)
radon_aug <- broom.mixed::augment(m, radon) 

radon_aug <- radon_aug %>% 
  group_by(county) %>% 
  mutate(n = n(), mean = mean(log_radon)) %>% 
  ungroup()

# lattice::dotplot(ranef(m))

okay <- radon_aug$county %>% 
  fct_infreq() %>% 
  fct_count() 

# create_thresholder <- function(limit) {
#   function(x) {
#     x > limit
#   }
# }
# 
# over_100 <- Position(create_thresholder(100), okay$n, right = TRUE)
# over_50 <- Position(create_thresholder(50), okay$n, right = TRUE)
# over_25 <- Position(create_thresholder(25), okay$n, right = TRUE)
# over_10 <- Position(create_thresholder(10), okay$n, right = TRUE)
# over_5 <- Position(create_thresholder(5), okay$n, right = TRUE)


p0 <- ggplot(radon_aug) + 
  aes(x = fct_infreq(county)) + 
  geom_bar() + 
  labs(x = NULL, y = "n") + 
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank()
  ) 
  

p1 <- ggplot(radon_aug) + 
  aes(x = fct_infreq(county), y = log_radon) + 
  stat_summary(
    fun.data = mean_se,
    color = "grey50", 
    fatten = 2
  ) + 
  geom_point(aes(y = .fitted), color = "blue") + 
  labs(x = "county", y = "log(radon)") +
  geom_hline(
    yintercept = fixef(m)[1]
  ) +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank()
  ) 
library(patchwork)
p0 + p1 + plot_layout(ncol = 1, heights = c(1, 4))  

vars <- broom.mixed::tidy(m, effects = "ran_pars", scales = "vcov")
var_g <- vars$estimate[1]
var_y <- vars$estimate[2]

# another try: variance ratios

# icc
var_g / (var_y + var_g)

# between group information is like this many observations within a county
var_y / var_g

# I wanted to add funneling lines to show the effect but messed up

radon_aug <- radon_aug %>% 
  mutate(
    diff = mean - mean(log_radon),
    diff2 = mean - .fixed,
    # page 733 in CAR
    denom = 1 + ((var_y) / (n * var_g)),
    # page 76 in Davidian
    blup = mean - ((var_y) / (n * var_g + var_y)) * (mean - .fixed),
    blup1.8 = 1.8 - ((var_y) / (n * var_g + var_y)) * (1.8 - .fixed)
  )

sum(radon_aug$.hat)
# sum(gam$edf)

ggplot(radon_aug) + 
  aes(x = n, y = diff2) + 
  geom_point() + 
  geom_line(aes(y = 1.8 - (1.8) / denom))

# ggplot(radon_aug) + 
#   aes(x = n, y = mean - .fitted) + 
#   geom_point() + 
#   geom_point(aes(y = .fitted - .fixed), color = "red") + 
#   geom_point(aes(y = blup - .fixed), color = "pink") + 
#   geom_line(aes(y = blup1.8))
# 
# Probably should replace mean with regression estimate.
ggplot(radon_aug) + 
  aes(x = n, y = mean - .fixed) + 
  geom_point() +
  scale_x_log10()

# I want to know the effective degrees of freedom.
# Wood p.83
# Paraphrase of book: suppose b ~ N(0, sigmab). How many dfs with b? If sigmab
# 0, then b does nothing. If sigmab is bigger and bigger, df = p groups, then we
# have a fixed effects model. "This suggests that the effective degrees of
# freedom for b should increase with sigma_b, from 0 up to p." 

# ?summary.gam() says that edf comes from the trace of the influence matrix.

icc <- var_g / (var_y + var_g)

var_y / var_g
deff = var_y / var_g

ns <- table(radon$county)
sum(ns / (1 + (ns - 1) * icc))

deff = 1 * (nobs(m) - 1) * icc
nobs(m) / deff

```


The first figure illustrates the observed county means and the estimated ones.
We see a classic example of partial pooling. For counties with many
observations, the estimate mean is hardly adjusted. For counties with less data,
the estimate is pulled towards the group mean. 

The contention behind the smooths = random effects claim is that what we just
did is a case of *smoothing*.


```{asis}
<blockquote class="twitter-tweet" data-conversation="none" data-lang="en" data-dnt="true" data-theme="light">
  <p lang="en" dir="ltr">Sadly, I feel like my career has peaked with the creation of this meme <a href="https://t.co/5ilRFonsy7">pic.twitter.com/5ilRFonsy7</a></p>

  <img src="/assets/images/spider-smooth.jpg" alt="Spiderman (Penalized smooths) pointing at (and being pointed at) by Spiderman (Random effects)" />
  
  &mdash; Eric Pedersen (@ericJpedersen) <a href="https://twitter.com/ericJpedersen/status/1293508069016637440?ref_src=twsrc%5Etfw">August 12, 2020</a>
</blockquote> 
```


### Smoothing example

Let's do a smooth with a generalized additive model.

```{r}
library(mgcv)
select <- dplyr::select

mcycle <- MASS::mcycle %>% 
  tibble::rowid_to_column()

mcycle_smooth <- gam(
  accel ~ 1 + s(times, bs = "cr", k = 15), 
  data = mcycle, 
  method = "ML"
)

mcycle$.fitted <- fitted(mcycle_smooth)

ggplot(mcycle) + 
  aes(x = times, y = accel) + 
  geom_point() + 
  geom_line(
    aes(y = .fitted), 
    color = "blue"
  )

```

So what happened here? We start with some splines. These splines are a bunch of
wiggly lines that are weighted and summed together to approximate some nonlinear
function. We are decomposing the `times` predictor into a bunch of individual
sub-trends that are weighted and summed together. My post on orthogonal
polynomial illustrates the same principle but with polynomial trends.

- [ ] link post


An easy way to pull the wiggles is to use the model matrix. 

<!-- The bumps here are a little pointy because the *x* values of line of the match -->
<!-- the data. -->

```{r}
trunc_mat(model.matrix(mcycle_smooth))

# Attach the `times` values onto the model matrix and convert to long format 
basis_long <- mcycle %>% 
  select(-rowid, -accel, -.fitted) %>% 
  cbind(model.matrix(mcycle_smooth)) %>% 
  distinct() %>%
  # Now have the spline values for each timepoint
  pivot_longer(c(-times))


ggplot(basis_long) + 
  aes(x = times, y = value, color = name) + 
  geom_line()



```

Now we can weight these by multiplying by the model coefficients. Here we use
the `diag(coef())` trick to prevent the weighted predictors from being summed together.


```{r}
basis_long <- mcycle %>% 
  select(-rowid, -accel, -.fitted) %>% 
  cbind(
    model.matrix(mcycle_smooth) %*% diag(coef(mcycle_smooth))
  ) %>% 
  distinct() %>%
  # Now have the spline values for each timepoint
  pivot_longer(c(-times))

ggplot(basis_long) + 
  aes(x = times, y = value, color = name) + 
  geom_line()
```

If we sum the lines together, we get the smoothed regression line.

```{r}
ggplot(basis_long) + 
  aes(x = times, y = value) + 
  # geom_point(aes(y = accel), data = mcycle) +
  geom_line(
    aes(group = name),
    color = "grey30"
  ) + 
  stat_summary(
    fun = "sum", 
    geom = "line", 
    color = "maroon", 
    size = 1
  ) 
```

What I have done so far is describe is regression with a basis function.
Smoothing splines go one step further: They penalize wiggliness to prevent
overfitting. The idea is as follows: We chose 15 knots for the last one. Where
did that number come from? What if we specified 30 knots? That's 30 predictors.
Isn't it really easy to overfit the data with this approach? 

Well, let's look at the 30-knot version.

```{r}
mcycle_smooth30 <- gam(
  accel ~ 1 + s(times, bs = "cr", k = 60), 
  data = mcycle, 
  method = "ML"
)

library(brms)
mcycle_smooth30_b <- brm(
  accel ~ 1 + s(times, bs = "cr", k = 60), 
  data = mcycle,
  backend = "cmdstanr"
)

mcycle_smooth10 <- gam(
  accel ~ 1 + s(times, bs = "cr", k = 60), 
  data = mcycle, 
  method = "ML", 
  sp = 10
)

smoothers <- scales::log_breaks(10)(c(1, 1000000))
coef(mcycle_smooth30)
mcycle_smooth30$sp
mcycle_smooth30$smooth[[1]] %>% str
mcycle_smooth10$smooth[[1]] %>% str
mcycle_smooth10$full.sp

mcycle_smooth30$smooth[[1]][["S"]]

model_work <- tibble(sp = c(smoothers, mcycle_smooth30$sp)) %>% 
  mutate(
    models = sp %>% map(function(sp) update(mcycle_smooth30, sp = sp)),
    .fitted = models %>% map(function(model) cbind(mcycle, .fit = fitted(model), .resid = residuals(model))),
    .deriv2 = models %>% map(gratia::derivatives, order = 2, newdata = mcycle),
    .penalty = models %>% map(
      function(m) {
        
        b <- coef(m)[-1]
        k <- length(b)
        d <- diff(diff(diag(k))) 
        s <- crossprod(d)
        as.vector(t(b) %*% s %*% b)
      }
    ),
    sp = models %>% map("sp")
  ) %>% 
  select(-models) %>% 
  unnest(.penalty)

models <- model_work %>% 
  select(-.deriv2) %>% 
  unnest(.fitted)

deriv2 <- model_work %>% 
  select(-.fitted) %>% 
  unnest(.deriv2)

?s
errors <- models %>% 
  group_by(sp) %>% 
  summarise(
    .resid = sum(.resid ^ 2), 
    .penalty = unique(.penalty),
    .groups = "drop"
  ) %>% 
  mutate(
    r = .resid + (1 / sqrt(sp)) * .penalty,
    r2 = .resid + sqrt(1 / sp) * .penalty
  )
errors


?smooth.construct
derivative <- deriv2 %>% 
  group_by(sp) %>% 
  summarise(.derivative = sum(derivative ^ 2))

errors %>% 
  left_join(derivative) %>% 
  mutate(
    .resid + .derivative
  )

ggplot(models) + 
  aes(x = times, y = .fit) + 
  geom_line(aes(color = sp, group = sp)) 

ggplot(models) + 
  aes(x = times, y = .fit) + 
  geom_line(aes(color = sp, group = sp))

mcycle$.fitted30 <- fitted(mcycle_smooth30)

summary()
mcycle_smooth30$sp

mcycle$.fitted30 <- fitted(mcycle_smooth30)

ggplot(mcycle) + 
  aes(x = times, y = accel) + 
  geom_point() + 
  # geom_line(aes(y = .fitted), color = "blue") + 
  geom_line(aes(y = .fitted30), color = "blue")
```

Huh, they hardly look any different. What's going on?

Penalties.


Behind the scenes, the model is




```{r}
tk <- seq(min(t),max(t),length=k) ## knotsX<-apply(diag(k),1,function(y) approx(tk,y,t,rule=2)$y)b <- lm(a ~ X-1)


d <- diff(diag(30), d = 2)
s <- crossprod(d) 
dim(s)
s <- t(d) %*% d
b <- coef(mcycle_smooth30)


mcycle_smooth30
mcycle_smooth30$smooth[[1]][["S"]]
sum(mcycle_smooth30$edf2)
sum(mcycle_smooth30$edf)

mcycle_smooth$sp
mcycle_smooth30$sp

gam.vcomp(mcycle_smooth)
gam.vcomp(mcycle_smooth30)
X <- model.matrix(mcycle_smooth30)
K <- 30
D <- diff(diff(diag(K))) 
## t(D)%*%D is penalty coef matrix
sp <- 2 ## square root smoothing parameter
XD <- rbind(X, D * sp)  ## augmented model matrix
y0 <- c(mcycle$accel,rep(0,nrow(D))) ## augmented data
b <- lm(y0~XD-1)     
## fit augmented model
plot(mcycle$times,mcycle$accel,ylab="accel",xlab="time")

lines(mcycle$times, X %*% coef(b),col=2,lwd=2)
```


















```{r, eval = FALSE}



# diff(mcycle$times)
# 
# # 

# 
# basis <- gratia::basis(s(times, bs = "cr", k = 15), mcycle, knots = NULL, constraints = TRUE)
# # # 
# 
# ggplot(mcycle) + geom_histogram(aes(times), binwidth = 5, boundary = 0)
# 
# ggplot(basis) +
#   aes(x = times, y = value, group = factor(bf)) +
#   geom_line()

# 

# # Construct the smoother
# sm <- smoothCon(s(times, bs = "cr", k = 15), data = mcycle, knots = NULL)[[1]]
# 
# times <- seq(min(mcycle$times), max(mcycle$times), length = 200)  
# 
# # Create matrix of spline values
# basis <- PredictMat(sm, data.frame(times = times))
# 
# dplyr::trunc_mat(basis)

predict(mcycle_smooth, newdata = data.frame(times), type = )

 


basis_long <- basis %>%
  as.data.frame() %>% 
  cbind(times, .) %>% 
  as_tibble() %>% 
  pivot_longer(c(-times))

ggplot(basis_long) + aes(x = times, y = value, color = name) + geom_line()
  















ggplot(basis_long) + aes(x = times, y = value, group = name) + geom_line()
  
nrow(model.matrix(mcycle_smooth))

basis_long_w <- mcycle %>% 
  cbind(model.matrix(mcycle_smooth) %*% diag(coef(mcycle_smooth))) %>% 
  as_tibble() %>%
  pivot_longer(c(-times, -accel, -rowid)) %>% 
  select(-rowid, -accel) %>% 
  distinct()




geom_line(
    data = . %>% 
      group_by(times, rowid) %>% 
      summarise(value = sum(value), .groups = "drop"),
    color = "blue", 
    size = 1
  )
  
#   
# ggplot(basis_long_w) + 
#   aes(x = times, y = value) + 
#   geom_line(aes(group = name)) + 
#   geom_line(
#     data = . %>% 
#       group_by(times, rowid) %>% 
#       summarise(value = sum(value), .groups = "drop"),
#     color = "blue", 
#     size = 1
#   )
```

Now we can look at the output from the mgcv summary.

```{r}
summary(mcycle_smooth)
gam.vcomp(mcycle_smooth)
```

```{r}
mcycle_smooth2 <- mgcv::gam(accel ~ s(times, bs = "cr", fx = TRUE), data = mcycle, method = "ML")

d2 <- gratia::derivatives(mcycle_smooth, order = 2, newdata = mcycle)
d2b <- gratia::derivatives(mcycle_smooth, order = 2)
summary(d2b)
sum(d2b)

summary(mcycle_smooth2)

ggplot(d2) + 
  aes(x = data, y = derivative) + 
  geom_line() + 
  geom_line(data = d2b)


```


Now games

Here is 


```{r}
library(mgcv)
m <- gam(log_radon ~ 1 + s(county, bs = "re"), data = radon, method = "ML")
summary(m)
gratia::observed_fitted_plot(m)




gratia::basis(s(county, bs = "re"), data = radon)
# radon_aug <- broom.mixed::augment(m, radon) 

mgcv::gam.vcomp(m)

```

(m is derivative penalized)



```{r}
# trying to write a naive stan model to do this.

n_fixed <- 1
n_knots <- 59
mcycle_smooth30_b
sm <- smoothCon(s(times, bs = "cr", k = 60), mcycle, absorb.cons = TRUE, modCon = 3, diagonal.penalty = TRUE)[[1]]

sm0 <- smoothCon(s(times, bs = "cr", k = 60), mcycle, absorb.cons = TRUE, modCon = 3, diagonal.penalty = FALSE)[[1]]


matplot(sm$X, type = "l")
matplot(sm$diagRP, type = "l")
matplot(sm, type = "l")

matplot(sm0$X, type = "l")
matplot(sm0$S[[1]], type = "l")
matplot(sm0$X %*% sm0$S[[1]], type = "l")

dim(sm0$X)
dim(sm0$S[[1]])


matplot(sm, type = "l")
str(sm)

sd <- brms::make_standata(accel ~ 1 + s(times, bs = "cr", k = 60), data = mcycle)

str(sm$S)
str(sm)

re <- smooth2random(sm, "", type = 2)
head(re$Xf) 
head(sd$Xs)

matplot(sm$diagRP, type = "l")

matplot(sd$Zs_1_1, type = "l")
matplot(apply(sd$Zs_1_1, 2, function(x) x + sd$Xs), type = "l")
matplot(sd$Xs, type = "l")
matplot(sd$, type = "l")




tf <- function(x,xk=seq(0,1,length=10),k=1) {
  ## generate kth tent function from set defined by xk
  yk <- xk*0;
  yk[k] <- 1
  approx(xk,yk,x)$y
}
tf.X <- function(x, xk=seq(0,1,length=10)) {
  ## tent function basis matrix given knot sequence 
  nk <- length(xk); 
  n <- length(x)
  X <- matrix(NA,n,nk)
  for (i in 1:nk) X[,i] <- tf(x,xk,i)
  X
}
K <- 40        
## basis dimension
t0 <- min(mcycle$times);
t1 <- max(mcycle$times)
tk=seq(t0,t1,length=K)        
## knot sequence
X <- tf.X(x=mcycle$times,xk=tk) 
## model matrix
b <- coef(lm(mcycle$accel~X-1)) 
## fit model
Xp <- tf.X(x=0:120/2,xk=tk)     
## prediction matrix
plot(mcycle$times,mcycle$accel,ylab="accel",xlab="time")
lines(0:120/2,Xp%*%b,col=2,lwd=2)

X0 <- sm$X
K <- 59
aug_d <- rbind(0, 0, diff(diag(K), difference = 2))
diag(aug_d) <- 1
aug_d

X <- t(backsolve(t(aug_d), t(X0)))
matplot(X0, type = "l")
matplot(aug_d, type = "l")
matplot(X, type = "l")
Z <- X[, -c(1,2)]
X <- X[, c(1:2)]
matplot(X, type = "l")
matplot(Z, type = "l")


make_standata()

sm <- smoothCon(s(times, bs = "cr", k = 60), mcycle, absorb.cons = TRUE, modCon = 3, diagonal.penalty = TRUE)[[1]]

re <- smooth2random(sm, "test", 2)

# random_effects
re$rand$Xr
# fixed effects
re$Xf
matplot(re$rand$Xr, type = "l")
matplot(re$Xf, type = "l")
matplot(re$trans.U, type = "l")
matplot(re$trans.D, type = "l")




data <- list(
  N = length(mcycle_smooth30$y),
  y = mcycle_smooth30$y,
  n_fixed = 2,
  n_knots = ncol(re$rand$Xr),
  spline_matrix = re$rand$Xr, 
  fixed_matrix = cbind(1, re$Xf)
)

m <- "
data {
  int<lower=0> N;               
  vector[N] y;                     
  int<lower=1> n_fixed;         
  int<lower=1> n_knots;         
  matrix[N,n_knots] spline_matrix;
  matrix[N,n_fixed] fixed_matrix;
}
parameters {
  real<lower=0> sigma_y;
  real<lower=0> sigma_knots;
  vector[n_fixed] beta_fixed;
  vector[n_knots] beta_knots;
}
model {
  sigma_y ~ normal(0, 100);
  beta_fixed ~ normal(0, 50);
  
  sigma_knots ~ normal(0, 100);
  beta_knots ~ normal(0, sigma_knots);

  y ~ normal(fixed_matrix * beta_fixed + spline_matrix * beta_knots, sigma_y);
}
"

# this doesn't seem to do any penalization. need to incorporate the penalty matrix?
# but how to do i get that? how do i incorporate it?

library(cmdstanr)
spline_mod <- cmdstan_model(write_stan_tempfile(m))

spline_fit <- spline_mod$sample(
  data = data,
  seed = 123,
  chains = 4,
  parallel_chains = 2,
  refresh = 500
)

spline_fit
gam.vcomp(mcycle_smooth30, rescale = FALSE)



# mcycle_smooth30_b$model
# mcycle_smooth30_b$stanvars
# mcycle_smooth30_b$data
# mcycle_smooth30_b$fit@inits
# mcycle_smooth30_b$fit@stanmodel

library(mgcv)
x <- runif(30)


sm <- smoothCon(s(times, bs = "cr", k = 8), mcycle, absorb.cons = TRUE, modCon = 3)[[1]]
matplot(sm$X, type = "l")

sm2 <- smoothCon(s(times, bs = "cr", k = 8), mcycle, absorb.cons = TRUE, modCon = 3, diagonal.penalty = TRUE)[[1]]
matplot(sm2$X, type = "l")

sm3 <- smooth2random(sm2, "", type = 2)
matplot(sm3$rand[[1]], type = "l")
matplot(sm3$rand, type = "l")

# this seems like a good lead but it feels like a cheat?
str(re)
# sm %*%

matplot(sm$X, type = "l")
matplot(sm$S[[1]], type = "l")
matplot(re$rand$Xr, type = "l")

matplot(sm$S[[1]], type = "l")


matplot(re$rand$Xr$X)
brms:::tidy_smef(mcycle_smooth30_b$formula, mcycle_smooth30_b$data)

brms:::standata_basis(mcycle_smooth30_b)

data_predictor(mcycle_smooth30_b$formula)

# where does brms find its data?
brms::standata(mcycle_smooth30_b)
model.matrix(mcycle_smooth30)

try <- rstan::read_stan_csv(spline_fit$output_files())
l <- loo(try)

summary(mcycle_smooth30)
gam.vcomp(mcycle_smooth30, rescale = FALSE)
```




```{r}
sm <- smoothCon(s(times, bs = "cr", k = 61), mcycle, absorb.cons = TRUE, modCon = 3)[[1]]

```


```{stan}
data {
  int<lower=0> N;               
  vector[N] y;                     
  int<lower=1> n_fixed;         
  int<lower=1> n_knots;         
  matrix[N,n_knots] spline_matrix;
  matrix[N,n_fixed] fixed_matrix;
}
parameters {
  real<lower=0> sigma_y;
  real<lower=0> sigma_knots;
  vector[n_fixed] beta_fixed;
  vector[n_knots] beta_knots;
}
model {
  sigma_y ~ normal(0, 100);
  beta_fixed ~ normal(0, 50);
  
  sigma_knots ~ normal(0, 100);
  beta_knots ~ normal(0, sigma_knots);

  y ~ normal(fixed_matrix * beta_fixed + spline_matrix * beta_knots, sigma_y);
}
```


```{r}

m_big <- "
data {
  int<lower=0> N;               // n data
  int<lower=0> y[N];            // data
  int<lower=1> L;               // groups
  int<lower=1,upper=L> ll[N];   // group membership
}
transformed data {
  real group_counts[L];

  group_counts = rep_array(0.0, L);

  for (i in 1:N) {
    group_counts[ll[i]] += ll[i];
  }

  real hm = mean(group_counts);

}
parameters {
  real<lower=0> sigma_y;
  real<lower=0> sigma_l;
  real grand_mean;
}
transformed parameters {
  real var_between = square(sigma_l);
  real var_within = square(sigma_y);
  real var_total = var_within + var_between;
}
model {
  matrix[N,N] bigma;
  vector[N] e;
  vector[N] zeroes;

  zeroes = rep_vector(0.0, N);
  bigma = rep_matrix(0.0, N, N);

  sigma_l ~ normal(0, 50);
  sigma_y ~ normal(0, 50);
  grand_mean ~ normal(0, 100);

  for (i in 1:N) {
    for (j in 1:N) {
      if (ll[i] == ll[j]) {
        if (i == j) {
          bigma[i,j] = var_total;
        } else {
          // If you divide everything by var_total this becomes
          // var_between / var_total which is the proportion of
          // of variance explained by between-group variance which
          // is the ICC
          bigma[i,j] = var_between;
        }
      }
    }
  }

  e = to_vector(y) - grand_mean;
  e ~ multi_normal(zeroes, bigma);
}
generated quantities {
  // how can we recover the group means?
  real<lower = 0, upper = 1> icc;
  icc = var_between / var_total;
  real mean_1 = mean((to_vector(y) - grand_mean)[1:3]);
  real mean_2 = mean((to_vector(y) - grand_mean)[4:6]);

  // trying to do effective sample size
  real deff = 1 + (hm - 1) * icc;
}
"

```

