---
title: ICCBot comes online (2019-09-18)
excerpt: ''
tags:
  - r
  - shiny
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

"I feel like I've had an epiphany. All of the complexities in the data. It doesn't matter."
{: .page__lead}

That was me, reporting to my lab on how we handle a statistical problem.

We give children lots of speech and language tests, and we wanted to know how reliable our score were for a particular test. That was my task. 

The test is fairly conventional articulation
inventory. Children name pictures, and listeners score whether the child
correctly produced certain target sounds in the word. The words work through the
consonants, consonant cluster, and vowels of English in different positions. For
example, the sound /l/ is tested twice: in *ladder* (word-initially) and
in *ball* (word-finally). The test weights some sounds are more than others. The
vowel in *knife* is word 3 points, the final /f/ is worth .5 points. All told,
there are 67 sounds tested and producing all of the sounds correctly yields an
overall score of 100 points.

We had 2 graduate students each score 130 administrations of the test. We are talking abouts thousands

* Overall agreement percentage.
* Overall agreement percentage, weighting by item scores.
* Compute the argreement percentages by child, report the range. 
* Weighted version of the above.
* Correlations of the scores.
* Logistic mixed effects model with varying effects for child, item, and rater.

These all answer interesting and important questions. The last one definitely comes the closest to matching the data generating process. 

They both chose the same answer 90% of the time. If we weighted their agreement,
by the item weights, we got 91% agreement. But what question was this statistic
answering? Agreement tells us how frequently the two chose the same answer or how frequently they
both awarded a point. 

**Report the reliability of the score that is used in the analyses**. That's the epiphany.



```{r, eval = FALSE}
library(tidyverse)

readr::read_csv("./_R/data/sample-irr-scores.csv") %>% 
  select(rater_1, rater_2) %>% 
  irr::agree()

readr::read_csv("./_R/data/sample-irr-scores.csv") %>% 
  select(rater_1, rater_2) %>% 
  irr::maxwell()
  
summarise(
    mean(agree),
    weighted.mean(agree, w = item_points)
  )

readr::read_csv("./_R/data/sample-irr-scores.csv") %>% 
  summarise(
    mean(agree),
    weighted.mean(agree, w = item_points)
  )

readr::read_csv("./_R/data/sample-irr-scores.csv") %>% 
  group_by(test_id) %>% 
  summarise(
    mean(agree),
    weighted.mean(agree, w = item_points)
  )

readr::read_csv("./_R/data/sample-irr-scores.csv") %>% 
  group_by(test_id) %>% 
  summarise(
    rater_1 = sum(rater_1),
    rater_2 = sum(rater_2)
  ) %>% 
  select(rater_1, rater_2) %>%
  irr::kappa2()

readr::read_csv("./_R/data/sample-irr-scores.csv") %>% 
  select(rater_1, rater_2) %>%
  irr::kappam.fleiss()

readr::read_csv("./_R/data/sample-irr-scores.csv") %>% 
  group_by(test_id) %>% 
  summarise(
    rater_1 = sum(rater_1),
    rater_2 = sum(rater_2)
  ) %>% 
  select(rater_1, rater_2) %>%
  cor()

readr::read_csv("./_R/data/sample-irr-scores.csv") %>% 
  group_by(test_id) %>% 
  summarise(
    rater_1 = sum(rater_1),
    rater_2 = sum(rater_2)
  ) %>% 
  select(rater_1, rater_2) %>%
  cor()

readr::read_csv("./_R/data/sample-irr-scores.csv") %>% 
  group_by(test_id) %>% 
  summarise(
    rater_1 = sum(rater_1),
    rater_2 = sum(rater_2)
  ) %>% 
  select(rater_1, rater_2) %>%
  irr::icc(model = "twoway", type = "agreement")


```

irr::kappa()

it looks at the speech sounds of English

The test involves 67 judgements


I had been tasked by my lab to figure how reliability our scorers were. We had 
