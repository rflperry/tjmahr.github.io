---
title: "RStanARM basics: visualizing uncertainty in linear regression"
excerpt: Summarizing many, many lines of fit.
header:
  overlay_image: sleeping-raccoon-1280.jpg
  caption: "Photo credit: [**Lance Anderson**](https://unsplash.com/photos/QZwf5yNopUo)"
tags:
  - rstanarm
  - bayesian
---

```{r, echo = FALSE, results = 'hide', message = FALSE}
# I think this was getting cached when I included it in the cached chunk that
# fit the model.
suppressPackageStartupMessages(library("rstanarm"))
```

As part of my tutorial on RStanARM, I presented some examples of how to 
visualize the uncertainty in Bayesian linear regression models. This post is an 
expanded demonstration of the approaches I presented in the tutorial.


Data: Does brain mass predict how much mammals sleep in a day?
-------------------------------------------------------------------------------

Let's use the [mammal sleep dataset from ggplot2][ggplot2-mammals]. This dataset
contains the number of hours spent sleeping per day for 83 different species of
mammals along with each species' brain mass (kg) and body mass (kg), among other
measures. Here's a first look at the data.

```{r brain-sleep, fig.cap = "Brain mass by sleep hours. This plot looks terrible because the masses span many orders of magnitudes."}
library("dplyr", warn.conflicts = FALSE)
library("ggplot2")

# Preview sorted by brain/body ratio. I chose this sorting so that humans would
# show up in the preview.
msleep %>% 
  select(name, sleep_total, brainwt, bodywt, everything()) %>% 
  arrange(desc(brainwt / bodywt))

ggplot(msleep) + 
  aes(x = brainwt, y = sleep_total) + 
  geom_point()
```

Hmmm, not very helpful. We should put our measures on a log-10 scale. Also, 27
of the species don't have brain mass data, so we'll exclude those rows for the
rest of this tutorial.

Let's plot the log-transformed data. We will also label the points for some
example critters :cat: so that we can get some intuition about the data in this
scaling. (Plus, I wanted to try out the [annotation tips][r4ds-labels] from the 
R4DS book.)

```{r log-brain-sleep, fig.cap = "Brain mass by sleep hours, now with both on a log-10 scale. Some species have their data highlighted."}
msleep <- msleep %>% 
  filter(!is.na(brainwt)) %>% 
  mutate(log_brainwt = log10(brainwt), 
         log_bodywt = log10(bodywt), 
         log_sleep_total = log10(sleep_total))

# Create a separate dataframe of the species to highlight.
ex_mammals <- c("Domestic cat", "Human", "Dog", "Cow", "Rabbit",
                "Big brown bat", "House mouse", "Horse", "Golden hamster")

# We will give some familiar species shorter names
renaming_rules <- c(
  "Domestic cat" = "Cat", 
  "Golden hamster" = "Hamster", 
  "House mouse" = "Mouse")

ex_points <- msleep %>% 
  filter(name %in% ex_mammals) %>% 
  mutate(name = stringr::str_replace_all(name, renaming_rules))

ggplot(msleep) + 
  aes(x = brainwt, y = sleep_total) + 
  geom_point(color = "grey40") +
  geom_point(size = 3, shape = 1, color = "grey40", data = ex_points) +
  ggrepel::geom_text_repel(aes(label = name), data = ex_points) + 
  scale_x_log10(breaks = c(.001, .01, .1, 1)) + 
  labs(x = "Brain mass (kg., log-scaled)", y = "Sleep per day (hours)")
```

As a child growing up on a dairy farm :cow:, it was remarkable to me how I
little saw cows sleeping, compared to dogs or cats. Were they okay? Are they
constantly tired and groggy? Maybe they are asleep when I'm asleep? Looks like
they just don't need very much sleep.

Next, let's fit a classical regression model. We will use a log-scaled sleep
measure so that the regression line doesn't imply negative sleep (even though
brains never get _that_ large).

```{r}
m1_classical <- lm(log_sleep_total ~ log_brainwt, data = msleep) 
summary(m1_classical)
```

```{r, echo = FALSE}
b <- coef(m1_classical)[1] %>% round(2)
m <- coef(m1_classical)[2] %>% round(2)
```

We can interpret the model in the usual way: A mammal with 1 kg (0 log-kg) 
of brain mass sleeps 10<sup>`r b`</sup>&nbsp;= `r round(10 ^ b, 1)` hours per
day. A mammal with a tenth of that brain mass (-1 log-kg) sleeps 
10<sup>`r b`&nbsp;+ `r -m`</sup>&nbsp;= `r round(10 ^ (b - m), 1)` hours.

Now, we illustrate the regression results to show the predicted mean of _y_ and
its 95% confidence interval. This task is readily accomplished in ggplot2 using 
`stat_smooth()`. This function fits a model and plots the mean and CI for each 
aesthetic grouping of data[^1] in a plot.

```{r log-brain-sleep-lm-fit, fig.cap = "Brain mass by sleep hours, log-10 scale, plus the predicted mean and 95% CI from a linear regression."}
ggplot(msleep) + 
  aes(x = log_brainwt, y = log_sleep_total) + 
  geom_point() +
  stat_smooth(method = "lm", level = .95)
```

This interval conveys some uncertainty in the estimate of the mean, but this 
interval has [a frequentist interpretation][interval-interp] which can be
unintuitive for this sort of data.

Now, for the point of this post: **What's the Bayesian version of this kind of
visualization**? Specifically, we want to illustrate:

* Predictions from a regression model
* Some uncertainty about those predictions
* The raw data used to train the model



Option 1: The pile-of-lines plot
-------------------------------------------------------------------------------

The regression line in the classical plot is just one particular line. It's the
line of best fit that satisfies a least-squares or maximum-likelihood objective. 
Our Bayesian model estimates an entire distribution of plausible
regression lines. The first way to visualize our uncertainty is to plot our
own line of best fit along with a sample of other lines from the posterior
distribution of the model.

First, we fit a model with weakly informative priors.

```{r stanarm-model, results = 'hide', cache = TRUE}
library("rstanarm")

m1 <- stan_glm(
  log_sleep_total ~ log_brainwt, 
  family = gaussian(), 
  data = msleep, 
  prior = normal(0, 3),
  prior_intercept = normal(0, 3))
```

We now have 4,000 credible regressions lines for our data.

```{r}
summary(m1)
```

For models fit by RStanARM, `coef()` returns the median parameter values. We
can see that the intercept and slope of the median line is pretty close to the
classical model's intercept and slope. The median line serves as the "point
estimate" for our model: If we had to summarize the modeled relationship using
just a single number for each parameter, we can use the medians.

```{r}
coef(m1)
coef(m1_classical)
```

The way to visualize our model then is plot our point-estimate line plus a
sample of the other credible lines from our model. First, we create a data-frame
with all 4,000 regression lines.

```{r}
fits <- m1 %>% 
  as_data_frame %>% 
  rename(intercept = `(Intercept)`) %>% 
  select(-sigma)
fits
```

We now plot the 500 randomly sampled lines from our model with light, 
semi-transparent lines.

```{r pile-of-lines-plot, fig.cap = "Brain mass by sleep hours, log-10 scale, plus the median regression line and 500 random regressions lines sampled from the posterior."}
# aesthetic controllers
n_draws <- 500
alpha_level <- .15
col_draw <- "grey60"
col_median <-  "#3366FF"

ggplot(msleep) + 
  aes(x = log_brainwt, y = log_sleep_total) + 
  # Plot a random sample of rows as gray semi-transparent lines
  geom_abline(aes(intercept = intercept, slope = log_brainwt), 
              data = sample_n(fits, n_draws), color = col_draw, 
              alpha = alpha_level) + 
  # Plot the median values in blue
  geom_abline(intercept = median(fits$intercept), 
              slope = median(fits$log_brainwt), 
              size = 1, color = col_median) +
  geom_point()
```

Each of these light lines represents a credible prediction of the mean across 
the values of _x_. As these line pile up on top of each other, they create an
uncertainty band around our line of best fit. More plausible lines are more 
likely to be sampled, so the more plausible lines overlap and create a uniform
color around the median line. As we get farther away from the mean of _x_, the
lines start to fan out and we see very faint individual lines.

The advantage of this plot is that it is a direct visualization of posterior
samples---one line per sample. This approach has limitations, however. Lines for
subgroups require a little more effort to undo interactions. Also, the
regression lines span the whole _x_ axis which is not appropriate when subgroups
only use a portion of the x-axis. Finally, I haven't found good defaults for the
aesthetic options: The number of samples, the colors to use, and the
transparency level. One can lose of a lot time fiddling with those knobs.



Option 2: Mean and its 95% interval
-------------------------------------------------------------------------------

Another option is a direct port of the `stat_smooth()` plot: Draw a line of
best fit and the 95% uncertainty interval around it.

To limit the amount of the _x_ axis used by the lines, we're going to create a
sequence of 80 points along the range of the data.

```{r}
x_rng <- range(msleep$log_brainwt) 
x_steps <- seq(x_rng[1], x_rng[2], length.out = 80)
new_data <- data_frame(
  observation = seq_along(x_steps), 
  log_brainwt = x_steps)
new_data
```

The function `posterior_linpred` returns the model-fitted means for a data-frame
of new data. I say _means_ because the function computes 80 predicted means for
each sample from the posterior. The result is 4000 x 80 matrix of fitted means.

```{r}
pred_lin <- posterior_linpred(m1, newdata = new_data)
dim(pred_lin)
```

We are going to reduce this down to just a median and 95% interval around each 
point. I do some tidying to get the data into a long format (one row per fitted 
mean per posterior sample), and then do a table-join with the `observation` 
column I included in `new_data`. I store these steps in a function because I
have to do them again later in this post.

```{r}
tidy_predictions <- function(mat_pred, df_data, obs_name = "observation",
                             prob_lwr = .025, prob_upr = .975) {
  # Get data-frame with one row per fitted value per posterior sample
  df_pred <- mat_pred %>% 
    as_data_frame %>% 
    setNames(seq_len(ncol(.))) %>% 
    tibble::rownames_to_column("posterior_sample") %>% 
    tidyr::gather_(obs_name, "fitted", setdiff(names(.), "posterior_sample"))
  df_pred
  
  # Helps with joining later
  class(df_pred[[obs_name]]) <- class(df_data[[obs_name]])
  
  # Summarise prediction interval for each observation
  df_pred %>% 
    group_by_(obs_name) %>% 
    summarise(median = median(fitted),
              lower = quantile(fitted, prob_lwr), 
              upper = quantile(fitted, prob_upr)) %>% 
    left_join(df_data, by = obs_name)
}

df_pred_lin <- tidy_predictions(pred_lin, new_data)
df_pred_lin
```

We can do the line-plus-interval plot using `geom_ribbon()` for the uncertainty
band.

```{r posterior-linpred-plot, fig.cap = "Brain mass by sleep hours, log-10 scale, plus the median and 95% uncertainty interval for the model-predicted mean."}
ggplot(msleep) + 
  aes(x = log_brainwt, y = log_sleep_total) + 
  geom_ribbon(aes(ymin = lower, ymax = upper, y = median), data = df_pred_lin, 
              alpha = 0.4, fill = "grey60") + 
  geom_line(aes(y = median), data = df_pred_lin, colour = "#3366FF", size = 1) + 
  geom_point()
```

This plot is just like the `stat_smooth()` plot, except the interval here is
interpreted in terms of post-data probabilities: We're 95% certain---given the
data, model and our prior information---that the average sleep duration is
contained in this interval. 

Although the interpretation of the interval changes (compared to a classical 
confidence interval), its location barely changes at all. If we overlay a 
`stat_smooth()` layer onto this plot, we can see that two sets of intervals are 
virtually identical. With this much data and for this simple of a model, both
types of models can make very similar estimates.

```{r posterior-linpred-plot-and-smooth, fig.cap = "Previous line-plus-interval plot with the classical regression line and confidence interval overlaid."}
last_plot() + stat_smooth(method = "lm")
```

The previous plot illustrates one limitation of this approach: Pragmatically
speaking, `stat_smooth(0)` basically does the same thing, and we're
not taking advantage of the affordances provided by our model. This is why 
RStanARM, in a kind of amusing way, disowns `posterior_linpred()` in its 
documentation:

> This function is occasionally convenient, but it should be used sparingly. 
> Inference and model checking should generally be carried out using the 
> posterior predictive distribution (see `posterior_predict`).

And elsewhere:

> `posterior_predict` to draw from the posterior predictive distribution of the 
? outcome, which is almost always preferable.



Option 3: Mean and 95% interval for model-generated data
-------------------------------------------------------------------------------

The reason why `posterior_predict()` is preferable is that it uses more 
information from our model, namely the error term `sigma`. 
`poseterior_linpred()` predicts averages; `posterior_predict()` predicts new 
data. This _posterior predictive checking_ helps us confirm whether our model---a 
story of how the data could have been generated---can produce new data that
resembles our data.

Here, we can use the function we defined earlier to get prediction intervals.

```{r}
# Still a matrix with one row per posterior draw and one column per observation
pred_post <- posterior_predict(m1, newdata = new_data)
dim(pred_post)

df_pred_post <- tidy_predictions(pred_post, new_data)
df_pred_post
```

And plot the interval in the same way.

```{r}
ggplot(msleep) + 
  aes(x = log_brainwt, y = log_sleep_total) + 
  geom_ribbon(aes(ymin = lower, ymax = upper, y = median), data = df_pred_post, 
              alpha = 0.4, fill = "grey60") + 
  geom_line(aes(y = median), data = df_pred_post, colour = "#3366FF", size = 1) + 
  geom_point()
```

First, we can appreciate that this interval is much wider. That's because the
interval doesn't summarize a particular statistic (like an average) but all of
the observations that can generated by our model. Okay, not _all_ of the
observations---just the 95% most probable observations.

Next, we can also appreciate that the line and the ribbon are jagged, due to 
randomness from data simulation. Each prediction is a random number draw, and at
each value of _x_, we have 4000 such random draws. We computed a median and 95% 
interval at each _x_, but due to simulation randomness, these medians do not 
smoothly connect together in the plot. That's okay, because these fluctuations
are relatively small.

Finally, we can see that there are only two points outside of the interval. 
These appear to be the giant armadillo and the roe deer. These two represent the
main outliers for our model because they fall slight outside of the 95%
prediction interval. 

```{r}
msleep %>% 
  filter(between(log_brainwt, -1.25, -0.90)) %>% 
  arrange(log_sleep_total) %>% 
  select(name, sleep_total, log_sleep_total, log_brainwt)
```

This posterior prediction plot does reveal a shortcoming of our model, when
plotted in a different manner.

```{r}
last_plot() + 
  geom_hline(yintercept = log10(24), color = "grey50") + 
  geom_label(x = 0, y = log10(24), label = "24 hours")
```

One faulty consequence of how our model was specified is that it predicts that
some mammals sleep more than 24 hours per day. What a life.











```{r, eval = FALSE}
p <- ggplot(msleep) + 
  aes(x = log_brainwt, y = sleep_total) + 
  geom_point() +
  stat_smooth(method = "lm") +
  scale_y_log10(breaks = c(1:5 * 4))

summary(p)

ggplot(msleep) + 
  aes(x = log(brainwt), y = sleep_total) + 
  geom_point() +
  stat_smooth(method = "lm")

ggplot(msleep) + 
  aes(x = log(brainwt), y = log(sleep_total)) + 
  geom_point() +
  stat_smooth(method = "lm")


lm(sleep_total ~ log(bodywt), msleep) %>% summary

lm(sleep_total ~ log(brainwt), msleep) %>% summary
lm(log(sleep_total) ~ log(brainwt), msleep) %>% summary



posterior_vs_prior(m1, pars = c("(Intercept)", "log_brainwt"))
summary(m1)

fits <- as_data_frame(m1) %>% 
  rename(intercept = `(Intercept)`)

model_data <- model.frame(m1) %>% 
  tibble::rownames_to_column("rownumber")











 default_aes = aes(colour = "#3366FF", fill = "grey60", size = 1,
    linetype = 1, weight = 1, alpha = 0.4)
)









x_range <- range(model_data$log_brainwt, na.rm = TRUE)
x_steps <- seq(x_range[1], x_range[2], length.out = 100)
new_data <- data_frame(log_brainwt = x_steps) %>% 
  tibble::rownames_to_column("rownumber")

long_predictions <- posterior_linpred(m1, newdata = new_data) %>% 
  as_data_frame() %>% 
  tibble::rownames_to_column("simulation") %>% 
  tidyr::gather(rownumber, prediction, -simulation)

long_predictions <- long_predictions %>% 
  left_join(new_data)

quantile_df <- function(...) {
  quantile(...) %>% as.list() %>% tibble::as_data_frame()
}


intervals <- long_predictions %>% 
  group_by(rownumber, log_brainwt) %>% 
  do(quantile_df(.$prediction, probs = c(.025, .50, .975))) %>% 
  ungroup()
  

n_draws <- 500
ggplot(msleep) + 
  aes(x = log_brainwt, y = log_sleep_total) + 
  geom_ribbon(aes(ymin = `2.5%`, y = `50%`, ymax = `97.5%`), data = intervals, color = "grey70", alpha = .5) + 
  geom_line(aes(y = `50%`), data = intervals, color = "blue", size = 1) + 
  geom_point()


ggplot(msleep) + 
  aes(x = log_brainwt, y = log_sleep_total) + 
  geom_ribbon(aes(ymin = `2.5%`, y = `50%`, ymax = `97.5%`), data = intervals, color = "grey70", alpha = .5) + 
  geom_line(aes(y = `50%`), data = intervals, color = "blue", size = 1) + 
  geom_point()  + 
  geom_abline(aes(intercept = intercept, slope = log_brainwt), 
              data = sample_n(fits, n_draws), alpha = .04) + 
  geom_abline(intercept = coef(m1)[1], slope = coef(m1)[2], 
              size = 1.25, color = "blue")
  
  
StatLm <- ggproto("StatLm", Stat, 
  required_aes = c("x", "y"),
  
  compute_group = function(data, scales) {
    rng <- range(data$x, na.rm = TRUE)
    grid <- data.frame(x = rng)
    
    mod <- lm(y ~ x, data = data)
    grid$y <- predict(mod, newdata = grid)
    
    grid
  }
)

stat_lm <- function(mapping = NULL, data = NULL, geom = "line",
                    position = "identity", na.rm = FALSE, show.legend = NA, 
                    inherit.aes = TRUE, ...) {
  layer(
    stat = StatLm, data = data, mapping = mapping, geom = geom, 
    position = position, show.legend = show.legend, inherit.aes = inherit.aes,
    params = list(na.rm = na.rm, ...)
  )
}

data <- msleep %>% 
  rename(y = log_sleep_total, x = log_brainwt)



StatStanGlm <- ggproto("StatStanGlm", Stat, 
  required_aes = c("x", "y"),
  
  compute_group = function(data, scales, params, n = 100, formula = y ~ x, family = gaussian()) {
    if (length(unique(data$x)) < 2) {
      # Not enough data to perform fit
      return(data.frame())
    }

    rng <- range(data$x, na.rm = TRUE)
    grid <- data.frame(x = seq(rng[1], rng[2], length = n))
    
    mod <- stan_glm(formula, family, data = data)
    predicted <- posterior_linpred(mod, newdata = grid)
    grid$y <- as.vector(apply(predicted, 2, median))
    grid
    
  }
)

stat_stan_glm <- function(mapping = NULL, data = NULL, geom = "line",
                    position = "identity", na.rm = FALSE, show.legend = NA, 
                    inherit.aes = TRUE, n = 50, formula = y ~ x, 
                    ...) {
  layer(
    stat = StatStanGlm, data = data, mapping = mapping, geom = geom, 
    position = position, show.legend = show.legend, inherit.aes = inherit.aes,
    params = list(n = n, formula = formula, na.rm = na.rm, ...)
  )
}



ggplot(mpg, aes(displ, hwy, color = factor(cyl))) + 
  geom_point() + 
  stat_stan_glm(formula = y ~ x)

 
  stat_glm(formula = y ~ poly(x, 10), geom = "point", colour = "red", n = 20)




ggplot(mpg, aes(displ, hwy)) + 
  geom_point() + 
  stat_lm()



long_predictions %>% 
  group_by(row_number) %>% 
  summarise()





poster

huron <- data.frame(year = 1875:1972, level = as.vector(LakeHuron))
h <- ggplot(huron, aes(year))

h + geom_ribbon(aes(ymin=0, ymax=level))
h + geom_area(aes(y = level))

h +
  geom_ribbon(aes(ymin = level - 1, ymax = level + 1), fill = "grey70") +
  geom_line(aes(y = level))

geom_ribbon()

tibble::has_rownames(mtcars)
tibble::has_rownames(iris)

ggplot(msleep) + 
  aes(x = log_brainwt, y = log_sleep_total) + 
  geom_point() +
  stat_smooth(method = "lm")

n_draws <- 500
ggplot(msleep) + 
  aes(x = log_brainwt, y = log_sleep_total) + 
  geom_abline(aes(intercept = intercept, slope = log_brainwt), 
              data = sample_n(fits, n_draws), alpha = .04) + 
  geom_abline(intercept = coef(m1)[1], slope = coef(m1)[2], 
              size = 1.25, color = "blue") + 
  geom_point()








ggplot(msleep) + 
  aes(x = log(bodywt), y = sleep_total) + 
  geom_point()
ggplot(msleep) + 
  aes(x = log(brainwt), y = sleep_total) + 
  geom_point()

ggplot(msleep) + 
  aes(x = log(brainwt), y = sleep_total) + 
  geom_point()

ggplot(msleep) + 
  aes(x = log(brainwt), y = sleep_total) + 
  geom_point()


msleep %>% filter(order == "Primates")

ggplot(msleep) + 
  aes(x = log(brainwt), y = sleep_rem) + 
  geom_point()

```


```{r}

```

```{r}

```

[^1]: That is, if we map the plot's color aesthetic to a categorical
variable in the data, `stat_smooth()` will fit a separate model for each
color/category. I figured this out when I tried to write my own function
`stat_smooth_stan()` based on [ggplot2's extensions vignette](https://cran.r-project.org/web/packages/ggplot2/vignettes/extending-ggplot2.html) 
and noticed that RStanARM was printing out MCMC sampling information for each
color/category of the data. 



[ggplot2-mammals]: https://vincentarelbundock.github.io/Rdatasets/doc/ggplot2/msleep.html
[r4ds-labels]: r4ds.had.co.nz/graphics-for-communication.html#annotations
[interval-interp]: https://stats.stackexchange.com/questions/2272/whats-the-difference-between-a-confidence-interval-and-a-credible-interval
