---
title: Draft post (2021-02-23)
excerpt: ''
tags: ''
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(eval = TRUE)
library(tidyverse)
library(brms)
library(mgcv)
```

*update date when published*

For a long time, I've been curious about something. It is a truth I've
seen casually dropped in textbooks, package documentation, and tweets:
random effects and penalized smoothing splines are the same thing. It sounds so profound and enlightened. What does it mean? How are they the same? What deep statistical *gnosis* was I missing out on?

I have spent months, off and on, trying to understand this equivalence.
I can't give you the full mathematical treatment, but I have the gist of
it and I can point you to the equations. In this post, I will try to 
highlight the connections between the two.


This post is an overview of what this assertion means.

Takeaways:

* Smoothing splines work by penalizing model coefficent to reduce the model degrees of freedom.
* Mixed effects models use partial pooling to strike a balance between a grand mean (complete pooling) and individual means (no pooling).

```{asis}
<blockquote class="twitter-tweet" data-conversation="none" data-lang="en" data-dnt="true" data-theme="light">
  <p lang="en" dir="ltr">Sadly, I feel like my career has peaked with the creation of this meme <a href="https://t.co/5ilRFonsy7">pic.twitter.com/5ilRFonsy7</a></p>

  <img src="/assets/images/spider-smooth.jpg" alt="Spiderman (Penalized smooths) pointing at (and being pointed at) by Spiderman (Random effects)" />
  <br/>
  &mdash; Eric Pedersen (@ericJpedersen) <a href="https://twitter.com/ericJpedersen/status/1293508069016637440?ref_src=twsrc%5Etfw">August 12, 2020</a>
</blockquote> 
```




<!-- So -->
<!-- instead, I am going to focus one key feature of this relationship. That -->
<!-- you can express smooths with a mixed effects model. -->




















## As short as possible


A smoothing spline tries to find model efficients that minimize model error and wiggliness:

$$
\begin{align*}
   y &= \mathbf{X}β + \epsilon \\ 
   \hat{β} &= \operatorname{arg min}_β\ \|\mathbf{y} − \mathbf{X}β\|^2 + \lambda\beta^\intercal\mathbf{S}\beta  \\
   &\ \text{(want }\hat{β} \text{ that minimizes fitting error and wiggliness)}\\
   \|\mathbf{y} − \mathbf{X}β\|^2 &: \text{sum of squared errors (minimize error to improve fit)} \\
   \lambda\beta^\intercal\mathbf{S}\beta &: \text{wiggliness penalty} \\
   \mathbf{X}β &: \text{spline basis times weights} \\
\mathbf{S} &: \text{penalty matrix (defines wiggliness for the spline)} \\
   \lambda &: \text{smoothness parameter (increase to make penalty stronger)} \\

\end{align*}
$$


The short version is that you can transform the math behind a smoothing spline to have the same smoothing splines to have the 


```{asis}
<blockquote class="twitter-tweet" data-conversation="none" data-dnt="true"><p lang="en" dir="ltr">random effects and splines are _the same_ thing. See also <a href="https://t.co/LgZTzZimH0">https://t.co/LgZTzZimH0</a></p>&mdash; DavidLawrenceMiller (@millerdl) <a href="https://twitter.com/millerdl/status/846719376338407424?ref_src=twsrc%5Etfw">March 28, 2017</a></blockquote> 
```

## Mixed model review

Let's review what these things means. Mixed effects models,
[apparently](/another-mixed-effects-model-visualization/) the [main
focus](/plotting-partial-pooling-in-mixed-effects-models/) of [this
blog](/iccbot-comes-online/) over the years, are used to estimate
"random" or "varying" effects. Here is the classic equation set up:

$$
\mathbf{y} = \mathbf{X\beta} + \mathbf{Zb} + \mathbf{\epsilon} \\
\mathbf{b} \sim \textsf{Normal}(0, \sigma_b) \\
\mathbf{\epsilon} \sim \textsf{Normal}(0, \sigma_y) \\
\mathbf{X}: \textrm{fixed effects model matrix} \\
\mathbf{Z}: \textrm{random effects model matrix} \\
\sigma_b, \sigma_y : \textrm{variance components} \\
\sigma_b : \textrm{where the magic happens} \\
$$

The magic here is the *σ*<sub>*b*</sub>, as it ties all of the
individual effects in **b** under a common distribution. If
*σ*<sub>*b*</sub> were replaced with a fixed number like 10, then all
of the effects in **b** would be independent and unaware of each other:
There would be no pooling of information between the groups.

Consider the [`radon` dataset][radon] example from Gelman and Hill (2007). Radon measurements were
taken in Minnesota counties. We would like to estimate the average radon
measurement for each county. We have a repeated measures situation, and some
counties have more observations than others. We use a mixed effects
model to estimate a population distribution of county estimates. These
county-level estimates are randomly varying effects.

- link book

```{r}
library(tidyverse)
theme_set(theme_grey(base_size = 14))
library(brms)
radon <- rstanarm::radon

b_radon <- brm(
  log_radon ~ 1 + (1 | county), 
  radon, 
  family = gaussian, 
  file = "radon"
)
b_radon
```

Here `sd(Intercept)` corresponds to *σ*<sub>*b*</sub>.

We can plot the observed county means alongside the model estimated
ones. First, I do some wrangling so that the difference between observed
means and estimated means are computed for use later on.

```{r county-means,  fig.height = 4, fig.width = 6, fig.asp = NULL}
radon_aug <- radon %>%
  # add ns and means
  group_by(county) %>% 
  mutate(
    observed_mean = mean(log_radon),
    county_n = n()
  ) %>% 
  ungroup() %>% 
  # add fitted values
  tidybayes::add_fitted_draws(b_radon) %>% 
  mutate(
    observed_minus_model = observed_mean - .value 
  ) %>% 
  # summarize fitted values
  ggdist::median_qi(.value, observed_minus_model) 

radon_aug$type <- "mixed model estimates"
radon$type <- "observed means"

p1 <- ggplot(radon_aug) + 
  aes(x = fct_infreq(county), y = log_radon) +
  stat_summary(
    aes(color = type, shape = type),
    data = radon,
    fun = mean,
    geom = "point"
  ) +
  geom_point(
    aes(y = .value, color = type, shape = type)
  ) + 
  # want to include 0 in the figure
  geom_blank(aes(y = 0)) +
  labs(
    x = "county (in decreasing order by sample size)", 
    y = "log(radon)"
  ) +
  geom_hline(yintercept = fixef(b_radon)[1]) +
  scale_color_manual(values = c("blue", "grey40")) +
  labs(color = NULL, shape = NULL) +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    legend.title = element_blank(),
    legend.position = "top", 
    legend.direction = "horizontal",
    legend.justification = "left",
  ) 

p1
```

We see a classic example of partial pooling. For
counties with many observations, the estimate mean is hardly adjusted.
For counties with less data, the estimate is pulled towards the
population mean (`Intercept` in the summary above). 

The following plot shows difference in between the observed means and
the estimate means, subtracting the grey triangles from the blue squares
in the plot above.

```{r shrinkage-by-n, fig.width = 5, fig.height = 4, fig.asp = NULL, out.width = "66%"}
ggplot(radon_aug) + 
  aes(x = county_n, y = observed_minus_model) + 
  geom_point() +
  labs(
    x = "Number of observations in county",
    y = "Observed mean - estimated mean"
  ) 
```



The contention behind the *smooths = random effects* claim is that what we
just did is a case of *smoothing*. These random effects are, in a way, 
smoothed fixed effects.




## But what's smoothing?

Now let's walk through a generalized additive model to demonstrate a
penalized smoothing spline. That was a mouth full, but basically
additive models are like the smoothing expansion pack for the
standard linear model. We're still doing regression but our models can 
do nonlinear relationships more easily now.

I will walk through a basic example of how a spline's basis functions
are weighted to approximate a nonlinear trend, but this is not going to
be a full tutorial. Other people have made video introductions to
[additive models][gs-gam] or the [mgcv package][nr-gam]. I first
learned them from [a tutorial for linguists][ms-gam] and then from
[the MGCV textbook][mgcv-textbook], but there are [other resources
online][gam-resources].


We use the [`mcycle`][mcycle] dataset which gives the head
acceleration in a simulated motorcycle accident. We are going to fit a
model, plot the smooth from it, and then we are going to work through
what the model did.

```{r smooth-demo, fig.width = 5}
library(mgcv)

mcycle <- MASS::mcycle %>% 
  tibble::rowid_to_column()

# Fit the model
gam_20 <- gam(
  accel ~ 1 + s(times, bs = "cr", k = 20), 
  data = mcycle, 
  method = "REML"
)

mcycle$.fitted <- fitted(gam_20)

ggplot(mcycle) + 
  aes(x = times, y = accel) + 
  geom_point(alpha = .5) + 
  geom_line(aes(y = .fitted), color = "blue") + 
  labs(x = "time after impact [ms]", y = "acceleration [g]")
```

So what happened here? We will cover it visually.


### Splines are the sums of weighted wiggles

Let's look at the regression formula.

```{r}
formula(gam_20)
```

We told `gam()` to estimate `accel` using an intercept term and a smooth
term on the time predictor (`s(times, ...)`). Specifically, we created
our smooth using a cubic regression spline basis (`bs = "cr"`) with `k
= 20` - 1 curves. (I am not sure what is going on there.) Our model is
estimating a function by adding up smaller components called basis
functions, and the space that defines those components is the *basis*.
These basis functions are weighted and summed together to produce a
smooth trend called a *spline*. The name *splines* is inspired by
drafting splines which are flexible strips of wood that can be weighted
and anchored in place to make a nice curve.

To reiterate, conceptually, we are decomposing the `times` predictor
into a bunch of individual wiggly lines (basis functions), and these are
weighted and summed together to approximate some nonlinear function. My
post on [orthogonal polynomials](/polypoly-package-released/)
illustrates the same principle but with polynomial basis functions.
Richard McElreath provides [a friendly 30-minute introduction
splines][rs-splines] in a Bayesian model in his Statistical
Rethinking course. One line I appreciate from his description is that
with splines, we replace a predictor variable, like `times`, with a set
of "synthetic" predictor variables.

An easy way to pull the wiggles is to use the model matrix. 

```{r}
model.matrix(gam_20) %>% 
  tibble::trunc_mat(width = 72)
```

To visualize the matrix, I am using a helper function from my personal R
package for plotting matrices in ggplot2. What we see is `times` on the
*x* axis and one line for the intercept and for each of the basis functions.

```{r matplot1, fig.width = 6}
# Helper function to plot the lines of a matrix
ggmatplot <- tjmisc::ggmatplot

# Helper function to label on a theme_grey() plot
annotate_grey <- tjmisc::annotate_label_grey

ggmatplot(cbind(mcycle$times, model.matrix(gam_20)), x_axis_column = 1) +
  annotate_grey("intercept", 0, 1.02, size = 5) +
  annotate_grey("individual\nbasis\nfunctions", 0, .16, size = 5) + 
  expand_limits(y = 1.2)  + 
  labs(x = "time [ms]", title = NULL)
```

Now we can weight these by multiplying by the model coefficients. Here
we use the `diag(coef())` trick to prevent the weighted predictors from
being summed together.

```{r matplot2, fig.width = 6}
weighted_coefs <- model.matrix(gam_20) %*% diag(coef(gam_20))

ggmatplot(cbind(mcycle$times, weighted_coefs), x_axis_column = 1) +
  annotate_grey("weighted intercept", 35, -40, size = 5) +
  annotate_grey("weighted basis functions", 0, 26, size = 5) +
  labs(x = "time [ms]", title = NULL)
```

We can see the two main inflections points in the dataset now. The
functions around 20 ms and 30 ms have become very active in order to push the
spline away from 0 at those times.

If we sum the lines together, we get the regression line (the intercept
plus the smoothing spline).

```{r matplot3, fig.width = 6}
ggmatplot(
  cbind(mcycle$times, weighted_coefs), 
  x_axis_column = 1, 
  n_colors = 1
) + 
  stat_summary(
    aes(group = 1), 
    color = "maroon", 
    fun = sum, 
    geom = "line", 
    size = 1.5
  ) +
  annotate_grey("sum", 10, -70, size = 5, color = "maroon") +
  labs(x = "time [ms]", title = NULL)
```

Our plots demonstrates regression with basis functions, but smoothing
splines go one step further: They penalize wiggliness to prevent
overfitting. The idea is as follows: For the above demonstration, we
chose a 20-dimension spline basis (19 curves because 1 is removed for
identifiability `r emo::ji("confused")` apparently). But where did that
number 20 come from? Thin air. What if we specified 50 knots? That's 50
predictors (1 intercept and 49 basis functions). Isn't it really easy to
overfit the data with this approach?

Well, let's look at a 50-dimension version.

```{r}
gam_50 <- gam(
  accel ~ s(times, bs = "cr", k = 50),
  data = mcycle, 
  method = "REML"
)

mcycle$.fitted50 <- fitted.values(gam_50)

ggplot(mcycle) + 
  aes(x = times, y = accel) + 
  geom_point(alpha = .5) +
  geom_line(aes(y = .fitted, color = "20"), size = 1) +
  geom_line(aes(y = .fitted50, color = "50"), size = 1) + 
  labs(
    x = "time after impact [ms]", 
    y = "acceleration [g]", 
    color = "Dimension"
  )
```

Huh, they hardly look any different. There is *no* overfitting. What's
going on? **Wiggliness is being penalized.**


### Fit versus smoothness

Behind the scenes, the model is trying to balance two competing goals.
On the one hand we want to maximize the fit to the data. In linear
regression, this goal amounts to minimizing the sum of squared errors.
On the other hand, we want to minimize wiggliness (overfitting). In
penalized smoothing splines, this done by first describing a penalty matrix that defines
*wiggliness* for that spline basis. These two features are pitted
against each other in the following equation. 

$$
\begin{align*}
   \mathbf{\hat{β}} &= \operatorname{arg min}_\mathbf{β}\ \|\mathbf{y} − \mathbf{Xβ}\|^2 + \lambda\mathbf{β}^\intercal\mathbf{Sβ}  \\
   &\ \text{(want }\mathbf{\hat{β}} \text{ that minimizes fitting error and wiggliness)}\\
   \|\mathbf{y} − \mathbf{Xβ}\|^2 &: \text{sum of squared errors (minimize error to improve fit)} \\
   \lambda\beta^\intercal\mathbf{Sβ} &: \text{wiggliness penalty} \\
   \mathbf{Xβ} &: \text{spline basis times weights} \\
\mathbf{S} &: \text{penalty matrix (defines wiggliness for the spline)} \\
   \lambda &: \text{smoothness parameter (increase to make penalty stronger)} \\

\end{align*}
$$

Don't worry about the exact mathematics here: Just appreciate that error
is now paired with wiggliness, and wiggliness is controlled by a penalty
matrix **S** and a smoothness parameter *λ*. And yes, *wiggliness* is
the technical term. I based the following equations from [Simon Wood's
slides][sw-slides], which use the phrase "fit-wiggliness tradeoff".

For our purposes, we won't worry too much about the penalty matrix. For
this model, wiggliness is defined by using the second derivative of the
estimated spline. The first derivative measures the slope/steepness of
the curve along with *x*, and the second derivatives measures how much
the slope/steepness changes with *x*. Thus, wiggliness is the change in
slope, and the penalty matrix provides penalties for each of the model
coefficients related to this wiggliness. The excellent
[gratia][gratia] will plot the penalty matrix as a heat map. *x* and
*y* represent model coefficients (weights for the basis functions), so
along the main diagonal we see a penalty applied to each coefficient. In
the two off-diagonals, we see neighboring basis functions have their
weights jointly unpenalized or penalized.   

```{r penalty, fig.width = 7}
gam_20 %>% 
  gratia::penalty() %>% 
  gratia::draw()
```

The stiffest penalties are meted out out to the 5th and 6th basis
functions, I think because these two basis functions cover the most rows
in the dataset, but I'm not 100% confident in that explanation.



To see the overfitting in action, we can disable the smoothing penalty
in the above spline-comparison plot by used fixed (`fx = TRUE`) regression
splines. Now the model's main goal is to minimize the error, and
the 50-dimension spline basis gives the model many, many degrees of freedom.

```{r}
gam_20_fx <- gam(
  accel ~ s(times, bs = "cr", k = 20, fx = TRUE),
  data = mcycle, 
  method = "REML"
)

gam_50_fx <- gam(
  accel ~ s(times, bs = "cr", k = 50, fx = TRUE),
  data = mcycle, 
  method = "REML"
)

mcycle$.fitted20_fx <- fitted.values(gam_20_fx)
mcycle$.fitted50_fx <- fitted.values(gam_50_fx)

ggplot(mcycle) + 
  aes(x = times, y = accel) + 
  geom_point(alpha = .5) + 
  geom_line(aes(y = .fitted20_fx, color = "20"), size = 1) +
  geom_line(aes(y = .fitted50_fx, color = "50"), size = 1) + 
  labs(
    x = "time after impact [ms]", 
    y = "acceleration [g]", 
    color = "Dimension"
  )
```

Thus, when we disable the penalty, the 50-dimension splines is free to
wiggle all over the place.


### How much smoothing happened: Effective degrees of freedom

The smoothing parameter, *λ*, is a hyperparameter. It controls the
spline coefficients (basis function weights), and it is estimated from
the data. We can set the *λ* manually and crank it up. In this case, the
model tries to find the least wiggly curve that decreases modeling
error: A straight line.

```{r}
gam_20_sp <- gam(
  accel ~ s(times, bs = "cr", k = 20, sp = 10000000),
  data = mcycle, 
  method = "REML"
)

mcycle$.fitted20_sp <- fitted.values(gam_20_sp)

ggplot(mcycle) + 
  aes(x = times, y = accel) + 
  geom_point() +
  geom_line(aes(y = .fitted, color = "estimated"), size = 1) +
  geom_line(aes(y = .fitted20_fx, color = "no smoothing"), size = 1) +
  geom_line(aes(y = .fitted20_sp, color = "10,000,000"), size = 1) + 
    labs(
    x = "time after impact [ms]", 
    y = "acceleration [g]", 
    color = "Smoothing λ"
  )
```

We need some way to talk about how much smoothing took place. On the one
hand, we might treat each basis function as an independent predictor
that uses up a full degree of freedom in fitting the curve. On the other
hand, we might penalize the basis function weights so much that they
produce a straight line, and thus, the batch of predictors effectively
acts just like a single predictor variable would. That is, they are
effectively estimating a curve that has just 1-degree-of-freedom's worth
of action in it. And indeed this is how mgcv describes the smoothness of the models: It reports the *effective (or estimated) degrees of freedom* (EDFs) behind the smooth. When 

If we look at the model summary, we see that our 20-dimension basis smooth has an EDF of 11.78.

```{r}
summary(gam_20)
```

Similarly, our 50-dimension basis smooth has only 12.81 effective
degrees of freedom, but the unpenalized version uses all 49 of its basis
function curves and uses 49 degrees of freedom.

```{r}
gam_50

gam_50_fx
```


## The big trick: Turn λ into a random effect variance in a mixed model

Okay, so far, here's what we have:

  - A spline decomposes a predictor into a number of wiggly basis
    functions
  - A penalized spline adds a penalty term to the model to reduce
    wiggleness.
  - This penalty shrinks model coefficients so they use a smaller number
    of degrees of freedom used by the model.
  - The amount of smoothing is controlled by a hyperparameter *λ*.

We saw another hyperparameter earlier on in this post its job was to
pull individual parameter estimates closer to 0: *σ*<sub>b</sub>. Both
of these hyperparameters are estimated from the data and perform
shrinkage on a batch of related coefficients (random effects or basis
function weights). 

So, here's the big thing. If you do a bunch of linear algebra (as in
[slide 7 here][smoothness-slides]), you can express the smooth as a
mixed model:


$$
\begin{align*}
\mathbf{y} &= \mathbf{X}\mathbf{\beta} + \mathbf{Zb} + \mathbf{\epsilon} \\
\mathbf{b} &\sim \textsf{Normal}(0, \sigma/\lambda) \\
\mathbf{\epsilon} &\sim \textsf{Normal}(0,\sigma) \\
\mathbf{X}, \mathbf{Z} &: \textrm{matrices from transforming spline and penalty matrices} \\
\mathbf{X} &: \textrm{unpenalized (fixed) effect model matrix} \\
\mathbf{Z} &: \textrm{penalized (random) effects model matrix} \\
\lambda &: \textrm{smoothness parameter} \\
\sigma &: \textrm{residual variance component} \\
\end{align*}
$$

And right there, on the second line, we see the mixed effects magic
again: *σ*/*λ* = *σ*<sub>b</sub>: Model coefficients are related under a
common distribution so that they can share information with each other.
We can smuggle penalized smooths into the mixed effects framework.

## Simple random effects are category smoothers

On consequence of this relationship is that you can walk this relation
backwards: You can fit a simple random effects using a basis matrix and
penalty matrix. Indeed, mgcv provides an `"re"` smoother basis so we can
estimate our mixed model from above a smooth.

```{r}
gam_radon <- gam(
  log_radon ~ 1 + s(county, bs = "re"), 
  data = radon,
  method = "REML"
)
```

In this case, the basis matrix is just a single indicator variable for
county, and the penalty matrix is a diagonal as each county effect is
equally penalized.

```{r}
ggmatplot(model.matrix(gam_radon)[, -1])

gam_radon %>% 
  gratia::penalty() %>% 
  gratia::draw() + 
  theme(axis.text = element_blank())
```

What I think is think is the coolest feature of random intercepts as smooths is that what the effective degrees of freeom tells us:

```{r}
summary(gam_radon)
```



```{r}

```




## Consequence 1: You can turn splines into mixed models

mgcv provides this feature.

```{r}
sm_raw <- smoothCon(
  s(times, k = 20, bs = "cr"), 
  data = mcycle
  # don't worry about these
  # absorb.cons = TRUE,
  # diagonal.penalty = TRUE
)
sm <- sm_raw[[1]]


re <- smooth2random(sm, "", type = 2)
re <- smooth2random(sm, "", type = 2)
```

Here you can see the extent of the transformation.

```{r}
ggmatplot(sm$X) +  
ggmatplot(re$Xf) + ggmatplot(re$rand$Xr)
```

You probably won't use this feature.

```{r}
sm_raw <- smoothCon(
  s(times, k = 20, bs = "cr"), 
  data = mcycle,
  # don't worry about these
  absorb.cons = TRUE,
  diagonal.penalty = TRUE
)
sm <- sm_raw[[1]]
re <- smooth2random(sm, "", type = 2)
```

Here you can see the extent of the transformation.

```{r}
ggmatplot(sm$X) + 
  expand_limits(y = c(-30, 75)) + 
  ggmatplot(re$Xf) +   
  expand_limits(y = c(-30, 75)) + 
  ggmatplot(re$rand$Xr)
  expand_limits(y = c(-30, 75)) 
```

















### How many splines aim I using then?

The model summary for a GAM reports the EDF for *effective degrees of
freedom*. Both the 20- and 50-knot model have around 12--14 degrees of
freedom. Let's consider 


Let's talk about penalized.

```{r}
gam_20$edf
plot(gam_50$edf)

```



### We can think about random effects in smoothing terms

```{r}

```


### brms uses the same syntax for mixed models and smooths






















### The Stan code behind the model

Above I coverd



## A mixed model in Stan

Let's look at the Stan code used to fit the above model in Stan.

```{r}
pull_stan_code_block <- function(code, what) {
  lines <- stringr::str_split(code, "\n")[[1]]
  starting_line <- str_which(lines, paste0("^", what))
  offsets <- lines[starting_line:length(lines)] %>% 
    str_which("^\\}$")
  lines[seq(starting_line, starting_line + offsets[1] - 1)] %>% 
    paste0(collapse = "\n")
}
what <- "parameters"

```


The model uses the so-called noncentered parameterization where random effects are added to the grand mean. Richard Mcelreath does a friendly tour of it here. https://elevanth.org/blog/2017/09/07/metamorphosis-multilevel-model/

For a deeper dive, Michael Betancourt covers the topic in the context of debugging degenerate posteriors. https://betanalpha.github.io/assets/case_studies/hierarchical_modeling.html#24_Normal_Hierarchical_Models


https://mc-stan.org/docs/2_26/stan-users-guide/reparameterization-section.html#non-centered-parameterization

$$
\begin{align*}
   
   \text{radon}_i &\sim \mathsf{Normal}(\text{mean}_{\text{county}[i]}, \sigma)& \text{[x]} \\
   \text{mean}_{\text{county}[i]} &= \mu + \eta_{\text{county}[i]} & \text{[counties are population mean plus offset]} \\
   \alpha_{\text{county}[i]} &= \tau \cdot z_{\text{county}[i]}
 & \text{[offsets are z scores]} \\
   \mu &\sim \mathsf{StudentT}(\nu=3, 0, 2.5) & \text{[population mean]} \\
   z_{\text{county}[i]} &\sim \mathsf{Normal}(0, 1) & \text{[variability between counties]} \\
   \tau &\sim \mathsf{StudentT}(3, 0, 2.5) & \text{[scale of z scores]} \\
   \sigma &\sim \mathsf{StudentT}(3, 1.3, 2.5) & \text{[variability within counties]} \\
\end{align*}
$$

These equations are 


```{r}
cat(stancode(b_radon))
```






```{r}

aweights <- tibble(
    x  = coef(gam_50_fx)[-1],
    y  = coef(gam_50)[-1]
  )
  
weights <- bind_rows(
  tibble(b = coef(gam_50)[-1], k = seq_along(b), fx = FALSE),
  tibble(b = coef(gam_50_fx)[-1], k = seq_along(b), fx = TRUE)
)

weights <- bind_rows(
  tibble(b = coef(gam_20)[-1], k = seq_along(b), fx = FALSE),
  tibble(b = coef(gam_20_fx)[-1], k = seq_along(b), fx = TRUE)
)

ggplot(aweights) + aes(x = x, y = y - x) + geom_point() 

ggplot(weights) + aes(x = k, y = b, color = fx) + geom_point()

coef(gam50_fx)[-1]

coef(gam50)[-1]
coef(gam50_fx)[-1]

```



---------------

```{r}
mcycle <- MASS::mcycle

b_gam_20 <- brm(
  accel ~ s(times, bs = "cr", k = 20),
  data = mcycle, 
  family = gaussian,
  file = "b-gam-20"
)
summary(b_gam_20)
gam.vcomp(gam_20, rescale = FALSE)

summary(gam_20)

gam_20
b_cycle_code <- make_stancode(
  accel ~ s(times, bs = "cr", k = 20),
  data = mcycle, 
  family = gaussian
)
b_cycle_code
```


## Another crack at the idea

Recall that in my partial pooling post, I contrasted three models:

- a complete pooling model
- a no pooling model
- a partial pooling model

```{r}

```



```{r}

```





[sw-slides]: https://www.maths.ed.ac.uk/~swood34/mgcv/tampere/basis-penalty.pdf "PDF of Simon Wood's slides on Basis Penalty Smoothers"

[smoothness-slides]: https://www.maths.ed.ac.uk/~swood34/mgcv/tampere/smoothness.pdf "Simon Wood's smoothness selection slides"

[nr-gam]: https://youtu.be/q4_t8jXcQgc "Noam Ross - Nonlinear Models in R: The Wonderful World of mgcv"

[gs-gam]: https://youtu.be/Zxokd_Eqrcg?t=506 "Dr. Gavin Simpson - Learning When, Where, and by How Much, Things Change [Remote]"

[ms-gam]: https://arxiv.org/abs/1703.05339 "Generalised additive mixed models for dynamic analysis in linguistics: a practical introduction"

[gam-resources]: https://github.com/noamross/gam-resources "Resources for Learning About and Using GAMs in R"

[mgcv-textbook]: https://amzn.to/37PLa8W "An Amazon Affliate link to Simon Wood's GAM textbook" 

[radon]: `r downlit::autolink_url("rstanarm::radon")` "Documentation on the radon dataset"

[mcycle]: `r downlit::autolink_url("MASS::mcycle")` "Documentation on the mcycle dataset"

[rs-splines]: https://youtu.be/ENxTrFf9a7c?t=2226 "Statistical Rethinking Winter 2019 Lecture 04"
