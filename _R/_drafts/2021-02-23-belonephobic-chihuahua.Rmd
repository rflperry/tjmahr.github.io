---
title: Draft post (2021-02-23)
excerpt: ''
tags: ''
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(eval = TRUE)
library(tidyverse)
library(brms)
```

*update date when published*

For a long time, I've been curious about something. It is a truth casually
mentioned in textbooks, package documentation, and tweets: random effects and
penalized smoothing splines are the same thing. 

I have spent months, off and on, trying to understand the equivalence
well enough to illustrate it without a full mathematical treatment. So
instead, I am going to focus one key feature of this relationship. That
you can express smooths with a mixed effects model.

```{asis}
<blockquote class="twitter-tweet" data-conversation="none" data-dnt="true"><p lang="en" dir="ltr">random effects and splines are _the same_ thing. See also <a href="https://t.co/LgZTzZimH0">https://t.co/LgZTzZimH0</a></p>&mdash; DavidLawrenceMiller (@millerdl) <a href="https://twitter.com/millerdl/status/846719376338407424?ref_src=twsrc%5Etfw">March 28, 2017</a></blockquote> 
```

This post is an overview of what this assertion means.

Takeaways:

* Smoothing splines work by penalizing model coefficent to reduce the model degrees of freedom.
* Mixed effects models use partial pooling to strike a balance between a grand mean (complete pooling) and individual means (no pooling).
















## As short as possible


A smoothing spline tries to find model efficients that minimize model error and wiggliness:

$$
\begin{align*}
   y &= \mathbf{X}β + \epsilon \\ 
   \hat{β} &= \operatorname{arg min}_β\ \|\mathbf{y} − \mathbf{X}β\|^2 + \lambda\beta^\intercal\mathbf{S}\beta  \\
   &\ \text{(want }\hat{β} \text{ that minimizes fitting error and wiggliness)}\\
   \|\mathbf{y} − \mathbf{X}β\|^2 &: \text{sum of squared errors (minimize error to improve fit)} \\
   \lambda\beta^\intercal\mathbf{S}\beta &: \text{wiggliness penalty} \\
   \mathbf{X}β &: \text{spline basis times weights} \\
\mathbf{S} &: \text{penalty matrix (defines wiggliness for the spline)} \\
   \lambda &: \text{smoothness parameter (increase to make penalty stronger)} \\

\end{align*}
$$


The short version is that you can transform the math behind a smoothing spline to have the same smoothing splines to have the 


## Mixed model review

Let's review what these things means. Mixed effects models, apparently the main
focus of this blog over the years, are used to estimate "random" or "varying"
effects. Here is the classic equation set up:

$$
\mathbf{y} = \mathbf{X\beta} + \mathbf{Zb} + \mathbf{\epsilon} \\
\mathbf{b} \sim \textsf{Normal}(0, \sigma_b) \\
\mathbf{\epsilon} \sim \textsf{Normal}(0, \sigma_y) \\
\mathbf{X}: \textrm{fixed effects model matrix} \\
\mathbf{Z}: \textrm{random effects model matrix} \\
\sigma_b, \sigma_y : \textrm{variance components} \\
\sigma_b : \textrm{where the magic happens} \\
$$

The magic here is the $\sigma_b$, as it ties all of the individual
effects in **b** under a common distribution. If $\sigma_b$ were
replaced with a fixed number like 10, then all of the effects in **b**
would be independent and unaware of each other. I've covered this ground
before.

Consider an example from Gelman and Hill (2007). Radon measurements were
taken in Minnesota counties. We would like to estimate the average radon
measurement for each county. We have a repeated measures situation, and some
counties have more observations than others. We use a mixed effects
model to estimate a population distribution of county estimates. These
county-level estimates are randomly varying effects.

```{r}
library(tidyverse)
library(brms)
radon <- rstanarm::radon

b_radon <- brm(
  log_radon ~ 1 + (1 | county), 
  radon, 
  family = gaussian, 
  file = "radon"
)
b_radon
```

Here `sd(Intercept)` corresponds to $\sigma_b$.

```{r double-fig, out.width = "100%", fig.height = 4, fig.width = 6}


radon_aug <- radon %>% 
  tidybayes::add_fitted_draws(b_radon) %>% 
  ggdist::median_qi() 

county_counts <- radon %>% 
  group_by(county) %>% 
  mutate(n = n(), mean = mean(log_radon)) %>% 
  ungroup()

p0 <- ggplot(county_counts) + 
  aes(x = fct_infreq(county)) + 
  geom_bar() + 
  labs(x = NULL, y = "n") + 
  theme_grey(base_size = 12) +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor = element_blank()
  ) 
  
radon_aug$type <- "mixed model estimates"
radon$type <- "observed means"

p1 <- ggplot(radon_aug) + 
  aes(x = fct_infreq(county), y = log_radon) +
  stat_summary(
    aes(color = type, shape = type),
    data = radon,
    fun = mean,
    geom = "point"
  ) +
  geom_point(
    aes(y = .value, color = type, shape = type)
  ) + 
  geom_blank(aes(y = 0)) +
  labs(x = "county", y = "log(radon)") +
  geom_hline(yintercept = fixef(b_radon)[1]) +
  scale_color_manual(values = c("blue", "grey40")) +
  labs(color = NULL, shape = NULL) +
  theme_grey(base_size = 12) +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    legend.position = c(.01, .01), 
    legend.direction = "horizontal",
    legend.justification = c(0, 0),
    legend.background = element_rect(fill = scales::alpha("grey93", .6))
  ) 

library(patchwork)
p0 + p1 + plot_layout(ncol = 1, heights = c(1.25, 5))  

```


The first figure illustrates the observed county means and the estimated ones.
We see a classic example of partial pooling. For counties with many
observations, the estimate mean is hardly adjusted. For counties with less data,
the estimate is pulled towards the population mean. 

The contention behind the smooths = random effects claim is that what we
just did is a case of *smoothing*.


```{asis}
<blockquote class="twitter-tweet" data-conversation="none" data-lang="en" data-dnt="true" data-theme="light">
  <p lang="en" dir="ltr">Sadly, I feel like my career has peaked with the creation of this meme <a href="https://t.co/5ilRFonsy7">pic.twitter.com/5ilRFonsy7</a></p>

  <img src="/assets/images/spider-smooth.jpg" alt="Spiderman (Penalized smooths) pointing at (and being pointed at) by Spiderman (Random effects)" />
  <br/>
  &mdash; Eric Pedersen (@ericJpedersen) <a href="https://twitter.com/ericJpedersen/status/1293508069016637440?ref_src=twsrc%5Etfw">August 12, 2020</a>
</blockquote> 
```


## Smoothing example

Now let's do a penalized smoothing spline with a generalized additive
model. Here we use the `mcycle` dataset which gives the head
acceleration in a simulated motorcycle accident. Here are going to fit a
model and plot the smooth from it. And then we are going to work through
what the model did.

- link to dataset

```{r}
library(mgcv)
select <- dplyr::select

mcycle <- MASS::mcycle %>% 
  tibble::rowid_to_column()

# Fit the model
gam_20 <- gam(
  accel ~ 1 + s(times, bs = "cr", k = 20), 
  data = mcycle, 
  method = "ML"
)

mcycle$.fitted <- fitted(gam_20)
ggplot(mcycle) + 
  aes(x = times, y = accel) + 
  geom_point() + 
  geom_line(
    aes(y = .fitted), 
    color = "blue"
  )
```

So what happened here? We will cover it visually.


### Prep work

We are going to plot a few matrices, so I need to define a helper
function that tidies a matrix and plots each column as a single line.
It's a ggplot2 port of `matplot()`

```{r}
ggmatplot <- function(x, focal_column = NULL, n_colors = 6) {
  ux <- unique(x)
  rownames(ux) <- seq_len(nrow(ux))

  # Figure out what to put on the x axis
  if (!is.null(focal_column)) {
    focal_label <- rlang::expr_label(substitute(x[, focal_column]))
  } else {
    ux <- cbind(seq_len(nrow(ux)), ux)
    focal_column <- 1
    focal_label <- "unique row number"
  }
  
  # Reshape non-x axis columns into a long dataframe
  long_ux <- reshape2::melt(
    ux[, -focal_column, drop = FALSE], 
    c(".row", ".column")
  )
  
  # Reshape axis column into a long dataframe
  focal <- reshape2::melt(
    ux[, focal_column, drop = FALSE], 
    c(".row", ".focal_name"), 
    value.name = focal_label
  )
  long_ux <- merge(long_ux, focal)
  
  # cycle through colors like matplot()
  column_numbers <- match(long_ux$.column, sort(unique(long_ux$.column)))
  long_ux$.color_cycle <- factor(column_numbers %% n_colors)
  
  ggplot(long_ux) + 
    aes(x = !! rlang::sym(focal_label), y = value, color = .color_cycle) + 
    geom_line(
      aes(group = .column)
    ) + 
    guides(color = FALSE) +
    scale_color_manual(
      values = unname(palette.colors(n_colors, palette = "R4"))
    ) +
    labs(title = rlang::expr_label(substitute(x)))
}

# Create a helper function annotating plots
annotate_grey <- function(label, x, y, size = 4, ...) {
  annotate(
    "label", x = x, y = y, label = label, size = size,
    hjust = 0, vjust = 0, fill = scales::alpha("grey93", .6), 
    label.size = 0, ...
  )
}
# xs <- sort(rnorm(100))
# polynomials <- poly(xs, 3)
# polynomials <- cbind(xs, polynomials)
# ggmatplot(polynomials, focal_column = 1)
```

## Splines are weighted wiggles


Let's look at the regression formula.

```{r}
formula(gam_20)
```

We told `gam()` to estimate `accel` using an
intercept term and a smooth on time `s(times, ...)` on the `times`
predictor. Specifically, we will create out smooth using cubic
regression splines basis (`bs = "cr"`) with `k = 20` knots. 

Conceptually, these splines are a bunch of wiggly lines that are
weighted and summed together to approximate some nonlinear function. We
are decomposing the `times` predictor into a bunch of individual
sub-trends that are weighted and summed together. My post on orthogonal
polynomial illustrates the same principle but with polynomial trends.

- [ ] link post

An easy way to pull the wiggles is to use the model matrix. 


```{r}
ggmatplot(cbind(mcycle$times, model.matrix(gam_20)), focal_column = 1) +
  annotate_grey("intercept", 0, 1.02, size = 5) +
  annotate_grey("individual\nsplines", 0, .56, size = 5) + 
  expand_limits(y = 1.1) 

# 
# sm <- smoothCon(s(times, bs = "cr", k = 20), data = mcycle)[[1]]
# 
# data = tibble(times = seq(min(mcycle$times), max(mcycle$times), length.out = 500))
# x <- PredictMat(sm, data)
# ggmatplot(PredictMat(sm, data) )
# # trying to find the raw spline basis
# 
# # penalty
# gam_20$smooth[[1]]$S[[1]] %>% ggmatplot() 
# # 
# 
# bf <- gratia::basis(s(times, bs = "cr", k = 30), data = mcycle)
# gratia::draw(bf)
# 
# bf <- gratia::basis(s(times, bs = "cr", k = 30, fx = TRUE), data = mcycle)
# gratia::draw(bf)

# ggmatplot(model.matrix(gam_20)) +
#   annotate_grey("intercept", 2.5, 1.05, size = 5) +
#   annotate_grey("splines at x values", 60, .66, size = 5)
# 
# m[, 1]


```

Now we can weight these by multiplying by the model coefficients. Here
we use the `diag(coef())` trick to prevent the weighted predictors from
being summed together.


```{r}
weighted_coefs <- model.matrix(gam_20) %*% diag(coef(gam_20))

ggmatplot(cbind(mcycle$times, weighted_coefs), focal_column = 1) +
  annotate_grey("weighted intercept", 35, -40, size = 5) +
  annotate_grey("weighted splines", 5, 26, size = 5)
```

If we sum the lines together, we get the regression line.

```{r}
ggmatplot(
  cbind(mcycle$times, weighted_coefs), 
  focal_column = 1, 
  n_colors = 1
) + 
  stat_summary(
    aes(group = 1), 
    color = "maroon", 
    fun = sum, 
    geom = "line", 
    size = 1.5
  ) +
  annotate_grey("sum", 10, -70, size = 5, color = "maroon")
```

What I have done so far is describe is regression with a basis function.
Smoothing splines go one step further: They penalize wiggliness to prevent
overfitting. The idea is as follows: We chose 20 knots for the last one. Where
did that number come from? What if we specified 50 knots? That's 50 predictors.
Isn't it really easy to overfit the data with this approach? 

Well, let's look at a 50-knot version.

```{r}
gam_50 <- gam(
  accel ~ s(times, bs = "cr", k = 50),
  data = mcycle, 
  method = "ML"
)

mcycle$.fitted50 <- fitted.values(gam_50)

ggplot(mcycle) + 
  aes(x = times, y = accel) + 
  geom_point() +
  geom_line(aes(y = .fitted, color = "20"), size = 1) +
  geom_line(aes(y = .fitted50, color = "50"), size = 1) + 
  labs(color = "Knots")
```

Huh, they hardly look any different. There is *no* overfitting. What's
going on? **Wiggliness is being penalized**


### Fit versus smoothness

Behind the scenes, the model is trying to balance two competing goals.
On the one hand we want to maximize the fit to the data. In linear
regression, this goal amounts to minimizing the sum of squared errors.
On the other hand, we want to minimize wiggliness (overfitting). In
GAMs, this done by first describing a penalty matrix that defines
*wiggliness* for that spline basis. These two features are pitted
against each other in the following equation.

$$
\begin{align*}
   \mathbf{\hat{β}} &= \operatorname{arg min}_\mathbf{β}\ \|\mathbf{y} − \mathbf{Xβ}\|^2 + \lambda\mathbf{β}^\intercal\mathbf{Sβ}  \\
   &\ \text{(want }\mathbf{\hat{β}} \text{ that minimizes fitting error and wiggliness)}\\
   \|\mathbf{y} − \mathbf{Xβ}\|^2 &: \text{sum of squared errors (minimize error to improve fit)} \\
   \lambda\beta^\intercal\mathbf{Sβ} &: \text{wiggliness penalty} \\
   \mathbf{Xβ} &: \text{spline basis times weights} \\
\mathbf{S} &: \text{penalty matrix (defines wiggliness for the spline)} \\
   \lambda &: \text{smoothness parameter (increase to make penalty stronger)} \\

\end{align*}
$$

And indeed, *wiggliness* is the technical term. I based the following
equations from [Simon Wood's slides][sw-slides], which use the phrase
"fit-wiggliness tradeoff".

For our purposes, we won't worry too much about the penalty matrix. For
this model, wiggleness is defined by using the second derivative of the
fitted curve. The first derivative measures the slope/steepness of the
curve along with *x*, and the second derivatives measures how much the
slope/steepness changes with *x*. Thus, wiggleness is the change in
slope, and the penalty matrix provides penalties for each of the model
coefficient related to this wiggleness. For instance, here the model
puts the stiffest penalties on the fifth spline. (I think presumably
because the dataset has a lot of datapoints in the narrow time region,
but I am not certain.)

```{r}
gam_20 %>% 
  gratia::penalty() %>% 
  gratia::draw()
```

To see the overfitting in action, we can disable the smoothing penalty
in the above knot-comparison plot by used fixed (`fx = TRUE`) regression
splines. Now the model's main goal is to minimize the error, and
the 50-knot spline basis gives the model many degrees of freedom to work
with.

```{r}
gam_20_fx <- gam(
  accel ~ s(times, bs = "cr", k = 20, fx = TRUE),
  data = mcycle, 
  method = "ML"
)

gam_50_fx <- gam(
  accel ~ s(times, bs = "cr", k = 50, fx = TRUE),
  data = mcycle, 
  method = "ML"
)

mcycle$.fitted20_fx <- fitted.values(gam_20_fx)
mcycle$.fitted50_fx <- fitted.values(gam_50_fx)

ggplot(mcycle) + 
  aes(x = times, y = accel) + 
  geom_point() +
  geom_line(aes(y = .fitted20_fx, color = "20"), size = 1) +
  geom_line(aes(y = .fitted50_fx, color = "50"), size = 1) + 
  labs(color = "Knots")
```


## The big trick: Turn λ into a random effect variance in a mixed model

The smoothing parameter, λ, is a hyperparameter. It controls the spline
coefficients, and it is estimated from the data. Crank up λ and the spline coefficients are smooshed into a
single linear effect. 


```{r}
gam_20_fx_sp <- gam(
  accel ~ s(times, bs = "cr", k = 20, sp = 10000000),
  data = mcycle, 
  method = "ML"
)
mcycle$.fitted20_fx_sp <- fitted.values(gam_20_fx_sp2)
ggplot(mcycle) + 
  aes(x = times, y = accel) + 
  geom_point() +
  geom_line(aes(y = .fitted20_fx, color = "estimated"), size = 1) +
  geom_line(aes(y = .fitted20_fx_sp, color = "10,000,000"), size = 1) + 
  labs(color = "λ")

```

We saw another hyperparameter in the mixed model part of this post: *&sigma;*<sub>b</sub>.

Now, here's the thing. If you do a bunch of linear algebra (as in [slide 7 here][smoothness-slides]), you can express the smooth as a mixed model:


$$
\begin{align*}

\mathbf{y} &= \mathbf{X}\mathbf{\beta} + \mathbf{Zb} + \mathbf{\epsilon} \\
\mathbf{b} &\sim \textsf{Normal}(0, \sigma/\lambda) \\
\mathbf{\epsilon} &\sim \textsf{Normal}(0,\sigma) \\
\mathbf{X}, \mathbf{Z} &: \textrm{matrices from transforming spline and penalty matrices} \\
\mathbf{X} &: \textrm{unpenalized (fixed) effect model matrix} \\
\mathbf{Z} &: \textrm{penalized (random) effects model matrix} \\
\lambda &: \textrm{smoothness parameter} \\
\sigma &: \textrm{residual variance component} \\
\end{align*}

$$

And right there, on the second line, we see the mixed effects magic
again: *σ*/*λ*: Model coefficients are related under a common
distribution so that they can share information with each other. 
Penalized smooths are randomly varying effects.


## Consequence 1: You can turn splines into mixed models

mgcv provides this feature.

```{r}
sm_raw <- smoothCon(
  s(times, k = 20, bs = "cr"), 
  data = mcycle
  # don't worry about these
  # absorb.cons = TRUE,
  # diagonal.penalty = TRUE
)
sm <- sm_raw[[1]]


re <- smooth2random(sm, "", type = 2)
re <- smooth2random(sm, "", type = 2)
```

Here you can see the extent of the transformation.

```{r}
ggmatplot(sm$X) +  
ggmatplot(re$Xf) + ggmatplot(re$rand$Xr)
```

You probably won't use this feature.

```{r}
sm_raw <- smoothCon(
  s(times, k = 20, bs = "cr"), 
  data = mcycle,
  # don't worry about these
  absorb.cons = TRUE,
  diagonal.penalty = TRUE
)
sm <- sm_raw[[1]]
re <- smooth2random(sm, "", type = 2)
```

Here you can see the extent of the transformation.

```{r}
ggmatplot(sm$X) + 
  expand_limits(y = c(-30, 75)) + 
  ggmatplot(re$Xf) +   
  expand_limits(y = c(-30, 75)) + 
  ggmatplot(re$rand$Xr)
  expand_limits(y = c(-30, 75)) 
```

















### How many splines aim I using then?

The model summary for a GAM reports the EDF for *effective degrees of
freedom*. Both the 20- and 50-knot model have around 12--14 degrees of
freedom. Let's consider 


Let's talk about penalized.

```{r}
gam_20$edf
plot(gam_50$edf)

```



### We can think about random effects in smoothing terms

```{r}

```


### brms uses the same syntax for mixed models and smooths






















### The Stan code behind the model

Above I coverd



## A mixed model in Stan

Let's look at the Stan code used to fit the above model in Stan.

```{r}
pull_stan_code_block <- function(code, what) {
  lines <- stringr::str_split(code, "\n")[[1]]
  starting_line <- str_which(lines, paste0("^", what))
  offsets <- lines[starting_line:length(lines)] %>% 
    str_which("^\\}$")
  lines[seq(starting_line, starting_line + offsets[1] - 1)] %>% 
    paste0(collapse = "\n")
}
what <- "parameters"

```


The model uses the so-called noncentered parameterization where random effects are added to the grand mean. Richard Mcelreath does a friendly tour of it here. https://elevanth.org/blog/2017/09/07/metamorphosis-multilevel-model/

For a deeper dive, Michael Betancourt covers the topic in the context of debugging degenerate posteriors. https://betanalpha.github.io/assets/case_studies/hierarchical_modeling.html#24_Normal_Hierarchical_Models


https://mc-stan.org/docs/2_26/stan-users-guide/reparameterization-section.html#non-centered-parameterization

$$
\begin{align*}
   
   \text{radon}_i &\sim \mathsf{Normal}(\text{mean}_{\text{county}[i]}, \sigma)& \text{[x]} \\
   \text{mean}_{\text{county}[i]} &= \mu + \eta_{\text{county}[i]} & \text{[counties are population mean plus offset]} \\
   \alpha_{\text{county}[i]} &= \tau \cdot z_{\text{county}[i]}
 & \text{[offsets are z scores]} \\
   \mu &\sim \mathsf{StudentT}(\nu=3, 0, 2.5) & \text{[population mean]} \\
   z_{\text{county}[i]} &\sim \mathsf{Normal}(0, 1) & \text{[variability between counties]} \\
   \tau &\sim \mathsf{StudentT}(3, 0, 2.5) & \text{[scale of z scores]} \\
   \sigma &\sim \mathsf{StudentT}(3, 1.3, 2.5) & \text{[variability within counties]} \\
\end{align*}
$$

These equations are 


```{r}
cat(stancode(b_radon))
```






```{r}

aweights <- tibble(
    x  = coef(gam_50_fx)[-1],
    y  = coef(gam_50)[-1]
  )
  
weights <- bind_rows(
  tibble(b = coef(gam_50)[-1], k = seq_along(b), fx = FALSE),
  tibble(b = coef(gam_50_fx)[-1], k = seq_along(b), fx = TRUE)
)

weights <- bind_rows(
  tibble(b = coef(gam_20)[-1], k = seq_along(b), fx = FALSE),
  tibble(b = coef(gam_20_fx)[-1], k = seq_along(b), fx = TRUE)
)

ggplot(aweights) + aes(x = x, y = y - x) + geom_point() 

ggplot(weights) + aes(x = k, y = b, color = fx) + geom_point()

coef(gam50_fx)[-1]

coef(gam50)[-1]
coef(gam50_fx)[-1]

```



---------------

```{r}
mcycle <- MASS::mcycle

b_gam_20 <- brm(
  accel ~ s(times, bs = "cr", k = 20),
  data = mcycle, 
  family = gaussian,
  file = "b-gam-20"
)
summary(b_gam_20)
gam.vcomp(gam_20, rescale = FALSE)

summary(gam_20)

gam_20
b_cycle_code <- make_stancode(
  accel ~ s(times, bs = "cr", k = 20),
  data = mcycle, 
  family = gaussian
)
b_cycle_code
```


## Another crack at the idea

Recall that in my partial pooling post, I contrasted three models:

- a complete pooling model
- a no pooling model
- a partial pooling model

```{r}

```



```{r}

```





[sw-slides]: https://www.maths.ed.ac.uk/~swood34/mgcv/tampere/basis-penalty.pdf "PDF of Simon Wood's slides on Basis Penalty Smoothers"

[smoothness-slides]: https://www.maths.ed.ac.uk/~swood34/mgcv/tampere/smoothness.pdf "Simon Wood's smoothness selection slides"
